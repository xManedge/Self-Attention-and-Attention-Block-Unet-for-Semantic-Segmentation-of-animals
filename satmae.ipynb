{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manedge/miniconda/envs/SemanticSegmentation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from rshf.satmae import SatMAE_Pre_MS\n",
    "from torchsummary import summary\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.952026Z",
     "start_time": "2025-09-12T21:28:25.818995Z"
    }
   },
   "id": "4361503907736dcb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.960512Z",
     "start_time": "2025-09-12T21:28:27.953260Z"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, mid_channels=None, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # If an intermediate channel count isn't provided, match the final width.\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channel\n",
    "\n",
    "        # Two consecutive Conv-BN-ReLU blocks with 3x3 kernels and padding=1 to keep H, W unchanged.\n",
    "        self.doubleconv = nn.Sequential(\n",
    "            # First 3x3 convolution: C_in -> mid_channels\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channel,\n",
    "                out_channels=mid_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,          # keep spatial dimensions (same-conv for 3x3)\n",
    "                bias=bias\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Second 3x3 convolution: mid_channels -> C_out\n",
    "            nn.Conv2d(\n",
    "                in_channels=mid_channels,\n",
    "                out_channels=out_channel,\n",
    "                kernel_size=3,\n",
    "                padding=1,          # keep spatial dimensions (same-conv for 3x3)\n",
    "                bias=bias\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.doubleconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TransformerSkipAdapter(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, scale_factor):\n",
    "        super().__init__()\n",
    "        # Upsample the skip feature to match decoder stage resolution\n",
    "        self.up = nn.Upsample(mode='bilinear', scale_factor=scale_factor, align_corners=True)\n",
    "        # Refine feature map after resizing\n",
    "        self.dc = DoubleConv(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, skip):\n",
    "        skip = self.up(skip)      # Resize to match decoder resolution\n",
    "        skip = self.dc(skip)      # Apply DoubleConv for refinement\n",
    "        return skip\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.960919Z",
     "start_time": "2025-09-12T21:28:27.955598Z"
    }
   },
   "id": "fd1acb1163d2412d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, scale_factor):\n",
    "        super().__init__()\n",
    "        # Reduce channels and refine features at current resolution\n",
    "        self.dc = DoubleConv(in_channel=in_channel, out_channel=out_channel)\n",
    "        # Upsample to next resolution for decoding\n",
    "        self.up = nn.Upsample(mode='bilinear', scale_factor=scale_factor, align_corners=True)\n",
    "\n",
    "    def forward(self, img_patch):\n",
    "        # Example: input = (B, 768, 14, 14)\n",
    "        img_patch = self.dc(img_patch)\n",
    "        # After DoubleConv: (B, 512, 14, 14)\n",
    "\n",
    "        img_patch = self.up(img_patch)\n",
    "        # After Upsample: (B, 512, 28, 28)\n",
    "\n",
    "        return img_patch\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.961054Z",
     "start_time": "2025-09-12T21:28:27.958719Z"
    }
   },
   "id": "a85a34c2a7ca2ea7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class outConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        # 1Ã—1 conv: channel-wise linear projection, preserves (H, W)\n",
    "        self.outconv = nn.Conv2d(in_channel, out_channel, kernel_size = 1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.outconv(image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.961993Z",
     "start_time": "2025-09-12T21:28:27.960842Z"
    }
   },
   "id": "8946dfdce4b759fb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 768, 12, 12]         197,376\n",
      "          Identity-2              [1, 144, 768]               0\n",
      "        PatchEmbed-3              [1, 144, 768]               0\n",
      "            Conv2d-4           [1, 768, 12, 12]         197,376\n",
      "          Identity-5              [1, 144, 768]               0\n",
      "        PatchEmbed-6              [1, 144, 768]               0\n",
      "            Conv2d-7           [1, 768, 12, 12]          99,072\n",
      "          Identity-8              [1, 144, 768]               0\n",
      "        PatchEmbed-9              [1, 144, 768]               0\n",
      "        LayerNorm-10              [1, 109, 768]           1,536\n",
      "           Linear-11             [1, 109, 2304]       1,771,776\n",
      "         Identity-12           [1, 16, 109, 48]               0\n",
      "         Identity-13           [1, 16, 109, 48]               0\n",
      "         Identity-14              [1, 109, 768]               0\n",
      "           Linear-15              [1, 109, 768]         590,592\n",
      "          Dropout-16              [1, 109, 768]               0\n",
      "        Attention-17              [1, 109, 768]               0\n",
      "         Identity-18              [1, 109, 768]               0\n",
      "         Identity-19              [1, 109, 768]               0\n",
      "        LayerNorm-20              [1, 109, 768]           1,536\n",
      "           Linear-21             [1, 109, 3072]       2,362,368\n",
      "             GELU-22             [1, 109, 3072]               0\n",
      "          Dropout-23             [1, 109, 3072]               0\n",
      "         Identity-24             [1, 109, 3072]               0\n",
      "           Linear-25              [1, 109, 768]       2,360,064\n",
      "          Dropout-26              [1, 109, 768]               0\n",
      "              Mlp-27              [1, 109, 768]               0\n",
      "         Identity-28              [1, 109, 768]               0\n",
      "         Identity-29              [1, 109, 768]               0\n",
      "            Block-30              [1, 109, 768]               0\n",
      "        LayerNorm-31              [1, 109, 768]           1,536\n",
      "           Linear-32             [1, 109, 2304]       1,771,776\n",
      "         Identity-33           [1, 16, 109, 48]               0\n",
      "         Identity-34           [1, 16, 109, 48]               0\n",
      "         Identity-35              [1, 109, 768]               0\n",
      "           Linear-36              [1, 109, 768]         590,592\n",
      "          Dropout-37              [1, 109, 768]               0\n",
      "        Attention-38              [1, 109, 768]               0\n",
      "         Identity-39              [1, 109, 768]               0\n",
      "         Identity-40              [1, 109, 768]               0\n",
      "        LayerNorm-41              [1, 109, 768]           1,536\n",
      "           Linear-42             [1, 109, 3072]       2,362,368\n",
      "             GELU-43             [1, 109, 3072]               0\n",
      "          Dropout-44             [1, 109, 3072]               0\n",
      "         Identity-45             [1, 109, 3072]               0\n",
      "           Linear-46              [1, 109, 768]       2,360,064\n",
      "          Dropout-47              [1, 109, 768]               0\n",
      "              Mlp-48              [1, 109, 768]               0\n",
      "         Identity-49              [1, 109, 768]               0\n",
      "         Identity-50              [1, 109, 768]               0\n",
      "            Block-51              [1, 109, 768]               0\n",
      "        LayerNorm-52              [1, 109, 768]           1,536\n",
      "           Linear-53             [1, 109, 2304]       1,771,776\n",
      "         Identity-54           [1, 16, 109, 48]               0\n",
      "         Identity-55           [1, 16, 109, 48]               0\n",
      "         Identity-56              [1, 109, 768]               0\n",
      "           Linear-57              [1, 109, 768]         590,592\n",
      "          Dropout-58              [1, 109, 768]               0\n",
      "        Attention-59              [1, 109, 768]               0\n",
      "         Identity-60              [1, 109, 768]               0\n",
      "         Identity-61              [1, 109, 768]               0\n",
      "        LayerNorm-62              [1, 109, 768]           1,536\n",
      "           Linear-63             [1, 109, 3072]       2,362,368\n",
      "             GELU-64             [1, 109, 3072]               0\n",
      "          Dropout-65             [1, 109, 3072]               0\n",
      "         Identity-66             [1, 109, 3072]               0\n",
      "           Linear-67              [1, 109, 768]       2,360,064\n",
      "          Dropout-68              [1, 109, 768]               0\n",
      "              Mlp-69              [1, 109, 768]               0\n",
      "         Identity-70              [1, 109, 768]               0\n",
      "         Identity-71              [1, 109, 768]               0\n",
      "            Block-72              [1, 109, 768]               0\n",
      "        LayerNorm-73              [1, 109, 768]           1,536\n",
      "           Linear-74             [1, 109, 2304]       1,771,776\n",
      "         Identity-75           [1, 16, 109, 48]               0\n",
      "         Identity-76           [1, 16, 109, 48]               0\n",
      "         Identity-77              [1, 109, 768]               0\n",
      "           Linear-78              [1, 109, 768]         590,592\n",
      "          Dropout-79              [1, 109, 768]               0\n",
      "        Attention-80              [1, 109, 768]               0\n",
      "         Identity-81              [1, 109, 768]               0\n",
      "         Identity-82              [1, 109, 768]               0\n",
      "        LayerNorm-83              [1, 109, 768]           1,536\n",
      "           Linear-84             [1, 109, 3072]       2,362,368\n",
      "             GELU-85             [1, 109, 3072]               0\n",
      "          Dropout-86             [1, 109, 3072]               0\n",
      "         Identity-87             [1, 109, 3072]               0\n",
      "           Linear-88              [1, 109, 768]       2,360,064\n",
      "          Dropout-89              [1, 109, 768]               0\n",
      "              Mlp-90              [1, 109, 768]               0\n",
      "         Identity-91              [1, 109, 768]               0\n",
      "         Identity-92              [1, 109, 768]               0\n",
      "            Block-93              [1, 109, 768]               0\n",
      "        LayerNorm-94              [1, 109, 768]           1,536\n",
      "           Linear-95             [1, 109, 2304]       1,771,776\n",
      "         Identity-96           [1, 16, 109, 48]               0\n",
      "         Identity-97           [1, 16, 109, 48]               0\n",
      "         Identity-98              [1, 109, 768]               0\n",
      "           Linear-99              [1, 109, 768]         590,592\n",
      "         Dropout-100              [1, 109, 768]               0\n",
      "       Attention-101              [1, 109, 768]               0\n",
      "        Identity-102              [1, 109, 768]               0\n",
      "        Identity-103              [1, 109, 768]               0\n",
      "       LayerNorm-104              [1, 109, 768]           1,536\n",
      "          Linear-105             [1, 109, 3072]       2,362,368\n",
      "            GELU-106             [1, 109, 3072]               0\n",
      "         Dropout-107             [1, 109, 3072]               0\n",
      "        Identity-108             [1, 109, 3072]               0\n",
      "          Linear-109              [1, 109, 768]       2,360,064\n",
      "         Dropout-110              [1, 109, 768]               0\n",
      "             Mlp-111              [1, 109, 768]               0\n",
      "        Identity-112              [1, 109, 768]               0\n",
      "        Identity-113              [1, 109, 768]               0\n",
      "           Block-114              [1, 109, 768]               0\n",
      "       LayerNorm-115              [1, 109, 768]           1,536\n",
      "          Linear-116             [1, 109, 2304]       1,771,776\n",
      "        Identity-117           [1, 16, 109, 48]               0\n",
      "        Identity-118           [1, 16, 109, 48]               0\n",
      "        Identity-119              [1, 109, 768]               0\n",
      "          Linear-120              [1, 109, 768]         590,592\n",
      "         Dropout-121              [1, 109, 768]               0\n",
      "       Attention-122              [1, 109, 768]               0\n",
      "        Identity-123              [1, 109, 768]               0\n",
      "        Identity-124              [1, 109, 768]               0\n",
      "       LayerNorm-125              [1, 109, 768]           1,536\n",
      "          Linear-126             [1, 109, 3072]       2,362,368\n",
      "            GELU-127             [1, 109, 3072]               0\n",
      "         Dropout-128             [1, 109, 3072]               0\n",
      "        Identity-129             [1, 109, 3072]               0\n",
      "          Linear-130              [1, 109, 768]       2,360,064\n",
      "         Dropout-131              [1, 109, 768]               0\n",
      "             Mlp-132              [1, 109, 768]               0\n",
      "        Identity-133              [1, 109, 768]               0\n",
      "        Identity-134              [1, 109, 768]               0\n",
      "           Block-135              [1, 109, 768]               0\n",
      "       LayerNorm-136              [1, 109, 768]           1,536\n",
      "          Linear-137             [1, 109, 2304]       1,771,776\n",
      "        Identity-138           [1, 16, 109, 48]               0\n",
      "        Identity-139           [1, 16, 109, 48]               0\n",
      "        Identity-140              [1, 109, 768]               0\n",
      "          Linear-141              [1, 109, 768]         590,592\n",
      "         Dropout-142              [1, 109, 768]               0\n",
      "       Attention-143              [1, 109, 768]               0\n",
      "        Identity-144              [1, 109, 768]               0\n",
      "        Identity-145              [1, 109, 768]               0\n",
      "       LayerNorm-146              [1, 109, 768]           1,536\n",
      "          Linear-147             [1, 109, 3072]       2,362,368\n",
      "            GELU-148             [1, 109, 3072]               0\n",
      "         Dropout-149             [1, 109, 3072]               0\n",
      "        Identity-150             [1, 109, 3072]               0\n",
      "          Linear-151              [1, 109, 768]       2,360,064\n",
      "         Dropout-152              [1, 109, 768]               0\n",
      "             Mlp-153              [1, 109, 768]               0\n",
      "        Identity-154              [1, 109, 768]               0\n",
      "        Identity-155              [1, 109, 768]               0\n",
      "           Block-156              [1, 109, 768]               0\n",
      "       LayerNorm-157              [1, 109, 768]           1,536\n",
      "          Linear-158             [1, 109, 2304]       1,771,776\n",
      "        Identity-159           [1, 16, 109, 48]               0\n",
      "        Identity-160           [1, 16, 109, 48]               0\n",
      "        Identity-161              [1, 109, 768]               0\n",
      "          Linear-162              [1, 109, 768]         590,592\n",
      "         Dropout-163              [1, 109, 768]               0\n",
      "       Attention-164              [1, 109, 768]               0\n",
      "        Identity-165              [1, 109, 768]               0\n",
      "        Identity-166              [1, 109, 768]               0\n",
      "       LayerNorm-167              [1, 109, 768]           1,536\n",
      "          Linear-168             [1, 109, 3072]       2,362,368\n",
      "            GELU-169             [1, 109, 3072]               0\n",
      "         Dropout-170             [1, 109, 3072]               0\n",
      "        Identity-171             [1, 109, 3072]               0\n",
      "          Linear-172              [1, 109, 768]       2,360,064\n",
      "         Dropout-173              [1, 109, 768]               0\n",
      "             Mlp-174              [1, 109, 768]               0\n",
      "        Identity-175              [1, 109, 768]               0\n",
      "        Identity-176              [1, 109, 768]               0\n",
      "           Block-177              [1, 109, 768]               0\n",
      "       LayerNorm-178              [1, 109, 768]           1,536\n",
      "          Linear-179             [1, 109, 2304]       1,771,776\n",
      "        Identity-180           [1, 16, 109, 48]               0\n",
      "        Identity-181           [1, 16, 109, 48]               0\n",
      "        Identity-182              [1, 109, 768]               0\n",
      "          Linear-183              [1, 109, 768]         590,592\n",
      "         Dropout-184              [1, 109, 768]               0\n",
      "       Attention-185              [1, 109, 768]               0\n",
      "        Identity-186              [1, 109, 768]               0\n",
      "        Identity-187              [1, 109, 768]               0\n",
      "       LayerNorm-188              [1, 109, 768]           1,536\n",
      "          Linear-189             [1, 109, 3072]       2,362,368\n",
      "            GELU-190             [1, 109, 3072]               0\n",
      "         Dropout-191             [1, 109, 3072]               0\n",
      "        Identity-192             [1, 109, 3072]               0\n",
      "          Linear-193              [1, 109, 768]       2,360,064\n",
      "         Dropout-194              [1, 109, 768]               0\n",
      "             Mlp-195              [1, 109, 768]               0\n",
      "        Identity-196              [1, 109, 768]               0\n",
      "        Identity-197              [1, 109, 768]               0\n",
      "           Block-198              [1, 109, 768]               0\n",
      "       LayerNorm-199              [1, 109, 768]           1,536\n",
      "          Linear-200             [1, 109, 2304]       1,771,776\n",
      "        Identity-201           [1, 16, 109, 48]               0\n",
      "        Identity-202           [1, 16, 109, 48]               0\n",
      "        Identity-203              [1, 109, 768]               0\n",
      "          Linear-204              [1, 109, 768]         590,592\n",
      "         Dropout-205              [1, 109, 768]               0\n",
      "       Attention-206              [1, 109, 768]               0\n",
      "        Identity-207              [1, 109, 768]               0\n",
      "        Identity-208              [1, 109, 768]               0\n",
      "       LayerNorm-209              [1, 109, 768]           1,536\n",
      "          Linear-210             [1, 109, 3072]       2,362,368\n",
      "            GELU-211             [1, 109, 3072]               0\n",
      "         Dropout-212             [1, 109, 3072]               0\n",
      "        Identity-213             [1, 109, 3072]               0\n",
      "          Linear-214              [1, 109, 768]       2,360,064\n",
      "         Dropout-215              [1, 109, 768]               0\n",
      "             Mlp-216              [1, 109, 768]               0\n",
      "        Identity-217              [1, 109, 768]               0\n",
      "        Identity-218              [1, 109, 768]               0\n",
      "           Block-219              [1, 109, 768]               0\n",
      "       LayerNorm-220              [1, 109, 768]           1,536\n",
      "          Linear-221             [1, 109, 2304]       1,771,776\n",
      "        Identity-222           [1, 16, 109, 48]               0\n",
      "        Identity-223           [1, 16, 109, 48]               0\n",
      "        Identity-224              [1, 109, 768]               0\n",
      "          Linear-225              [1, 109, 768]         590,592\n",
      "         Dropout-226              [1, 109, 768]               0\n",
      "       Attention-227              [1, 109, 768]               0\n",
      "        Identity-228              [1, 109, 768]               0\n",
      "        Identity-229              [1, 109, 768]               0\n",
      "       LayerNorm-230              [1, 109, 768]           1,536\n",
      "          Linear-231             [1, 109, 3072]       2,362,368\n",
      "            GELU-232             [1, 109, 3072]               0\n",
      "         Dropout-233             [1, 109, 3072]               0\n",
      "        Identity-234             [1, 109, 3072]               0\n",
      "          Linear-235              [1, 109, 768]       2,360,064\n",
      "         Dropout-236              [1, 109, 768]               0\n",
      "             Mlp-237              [1, 109, 768]               0\n",
      "        Identity-238              [1, 109, 768]               0\n",
      "        Identity-239              [1, 109, 768]               0\n",
      "           Block-240              [1, 109, 768]               0\n",
      "       LayerNorm-241              [1, 109, 768]           1,536\n",
      "          Linear-242             [1, 109, 2304]       1,771,776\n",
      "        Identity-243           [1, 16, 109, 48]               0\n",
      "        Identity-244           [1, 16, 109, 48]               0\n",
      "        Identity-245              [1, 109, 768]               0\n",
      "          Linear-246              [1, 109, 768]         590,592\n",
      "         Dropout-247              [1, 109, 768]               0\n",
      "       Attention-248              [1, 109, 768]               0\n",
      "        Identity-249              [1, 109, 768]               0\n",
      "        Identity-250              [1, 109, 768]               0\n",
      "       LayerNorm-251              [1, 109, 768]           1,536\n",
      "          Linear-252             [1, 109, 3072]       2,362,368\n",
      "            GELU-253             [1, 109, 3072]               0\n",
      "         Dropout-254             [1, 109, 3072]               0\n",
      "        Identity-255             [1, 109, 3072]               0\n",
      "          Linear-256              [1, 109, 768]       2,360,064\n",
      "         Dropout-257              [1, 109, 768]               0\n",
      "             Mlp-258              [1, 109, 768]               0\n",
      "        Identity-259              [1, 109, 768]               0\n",
      "        Identity-260              [1, 109, 768]               0\n",
      "           Block-261              [1, 109, 768]               0\n",
      "       LayerNorm-262              [1, 109, 768]           1,536\n",
      "          Linear-263              [1, 109, 512]         393,728\n",
      "       LayerNorm-264              [1, 433, 512]           1,024\n",
      "          Linear-265             [1, 433, 1536]         787,968\n",
      "        Identity-266           [1, 16, 433, 32]               0\n",
      "        Identity-267           [1, 16, 433, 32]               0\n",
      "        Identity-268              [1, 433, 512]               0\n",
      "          Linear-269              [1, 433, 512]         262,656\n",
      "         Dropout-270              [1, 433, 512]               0\n",
      "       Attention-271              [1, 433, 512]               0\n",
      "        Identity-272              [1, 433, 512]               0\n",
      "        Identity-273              [1, 433, 512]               0\n",
      "       LayerNorm-274              [1, 433, 512]           1,024\n",
      "          Linear-275             [1, 433, 2048]       1,050,624\n",
      "            GELU-276             [1, 433, 2048]               0\n",
      "         Dropout-277             [1, 433, 2048]               0\n",
      "        Identity-278             [1, 433, 2048]               0\n",
      "          Linear-279              [1, 433, 512]       1,049,088\n",
      "         Dropout-280              [1, 433, 512]               0\n",
      "             Mlp-281              [1, 433, 512]               0\n",
      "        Identity-282              [1, 433, 512]               0\n",
      "        Identity-283              [1, 433, 512]               0\n",
      "           Block-284              [1, 433, 512]               0\n",
      "       LayerNorm-285              [1, 433, 512]           1,024\n",
      "          Linear-286             [1, 433, 1536]         787,968\n",
      "        Identity-287           [1, 16, 433, 32]               0\n",
      "        Identity-288           [1, 16, 433, 32]               0\n",
      "        Identity-289              [1, 433, 512]               0\n",
      "          Linear-290              [1, 433, 512]         262,656\n",
      "         Dropout-291              [1, 433, 512]               0\n",
      "       Attention-292              [1, 433, 512]               0\n",
      "        Identity-293              [1, 433, 512]               0\n",
      "        Identity-294              [1, 433, 512]               0\n",
      "       LayerNorm-295              [1, 433, 512]           1,024\n",
      "          Linear-296             [1, 433, 2048]       1,050,624\n",
      "            GELU-297             [1, 433, 2048]               0\n",
      "         Dropout-298             [1, 433, 2048]               0\n",
      "        Identity-299             [1, 433, 2048]               0\n",
      "          Linear-300              [1, 433, 512]       1,049,088\n",
      "         Dropout-301              [1, 433, 512]               0\n",
      "             Mlp-302              [1, 433, 512]               0\n",
      "        Identity-303              [1, 433, 512]               0\n",
      "        Identity-304              [1, 433, 512]               0\n",
      "           Block-305              [1, 433, 512]               0\n",
      "       LayerNorm-306              [1, 433, 512]           1,024\n",
      "          Linear-307             [1, 433, 1536]         787,968\n",
      "        Identity-308           [1, 16, 433, 32]               0\n",
      "        Identity-309           [1, 16, 433, 32]               0\n",
      "        Identity-310              [1, 433, 512]               0\n",
      "          Linear-311              [1, 433, 512]         262,656\n",
      "         Dropout-312              [1, 433, 512]               0\n",
      "       Attention-313              [1, 433, 512]               0\n",
      "        Identity-314              [1, 433, 512]               0\n",
      "        Identity-315              [1, 433, 512]               0\n",
      "       LayerNorm-316              [1, 433, 512]           1,024\n",
      "          Linear-317             [1, 433, 2048]       1,050,624\n",
      "            GELU-318             [1, 433, 2048]               0\n",
      "         Dropout-319             [1, 433, 2048]               0\n",
      "        Identity-320             [1, 433, 2048]               0\n",
      "          Linear-321              [1, 433, 512]       1,049,088\n",
      "         Dropout-322              [1, 433, 512]               0\n",
      "             Mlp-323              [1, 433, 512]               0\n",
      "        Identity-324              [1, 433, 512]               0\n",
      "        Identity-325              [1, 433, 512]               0\n",
      "           Block-326              [1, 433, 512]               0\n",
      "       LayerNorm-327              [1, 433, 512]           1,024\n",
      "          Linear-328             [1, 433, 1536]         787,968\n",
      "        Identity-329           [1, 16, 433, 32]               0\n",
      "        Identity-330           [1, 16, 433, 32]               0\n",
      "        Identity-331              [1, 433, 512]               0\n",
      "          Linear-332              [1, 433, 512]         262,656\n",
      "         Dropout-333              [1, 433, 512]               0\n",
      "       Attention-334              [1, 433, 512]               0\n",
      "        Identity-335              [1, 433, 512]               0\n",
      "        Identity-336              [1, 433, 512]               0\n",
      "       LayerNorm-337              [1, 433, 512]           1,024\n",
      "          Linear-338             [1, 433, 2048]       1,050,624\n",
      "            GELU-339             [1, 433, 2048]               0\n",
      "         Dropout-340             [1, 433, 2048]               0\n",
      "        Identity-341             [1, 433, 2048]               0\n",
      "          Linear-342              [1, 433, 512]       1,049,088\n",
      "         Dropout-343              [1, 433, 512]               0\n",
      "             Mlp-344              [1, 433, 512]               0\n",
      "        Identity-345              [1, 433, 512]               0\n",
      "        Identity-346              [1, 433, 512]               0\n",
      "           Block-347              [1, 433, 512]               0\n",
      "       LayerNorm-348              [1, 433, 512]           1,024\n",
      "          Linear-349             [1, 433, 1536]         787,968\n",
      "        Identity-350           [1, 16, 433, 32]               0\n",
      "        Identity-351           [1, 16, 433, 32]               0\n",
      "        Identity-352              [1, 433, 512]               0\n",
      "          Linear-353              [1, 433, 512]         262,656\n",
      "         Dropout-354              [1, 433, 512]               0\n",
      "       Attention-355              [1, 433, 512]               0\n",
      "        Identity-356              [1, 433, 512]               0\n",
      "        Identity-357              [1, 433, 512]               0\n",
      "       LayerNorm-358              [1, 433, 512]           1,024\n",
      "          Linear-359             [1, 433, 2048]       1,050,624\n",
      "            GELU-360             [1, 433, 2048]               0\n",
      "         Dropout-361             [1, 433, 2048]               0\n",
      "        Identity-362             [1, 433, 2048]               0\n",
      "          Linear-363              [1, 433, 512]       1,049,088\n",
      "         Dropout-364              [1, 433, 512]               0\n",
      "             Mlp-365              [1, 433, 512]               0\n",
      "        Identity-366              [1, 433, 512]               0\n",
      "        Identity-367              [1, 433, 512]               0\n",
      "           Block-368              [1, 433, 512]               0\n",
      "       LayerNorm-369              [1, 433, 512]           1,024\n",
      "          Linear-370             [1, 433, 1536]         787,968\n",
      "        Identity-371           [1, 16, 433, 32]               0\n",
      "        Identity-372           [1, 16, 433, 32]               0\n",
      "        Identity-373              [1, 433, 512]               0\n",
      "          Linear-374              [1, 433, 512]         262,656\n",
      "         Dropout-375              [1, 433, 512]               0\n",
      "       Attention-376              [1, 433, 512]               0\n",
      "        Identity-377              [1, 433, 512]               0\n",
      "        Identity-378              [1, 433, 512]               0\n",
      "       LayerNorm-379              [1, 433, 512]           1,024\n",
      "          Linear-380             [1, 433, 2048]       1,050,624\n",
      "            GELU-381             [1, 433, 2048]               0\n",
      "         Dropout-382             [1, 433, 2048]               0\n",
      "        Identity-383             [1, 433, 2048]               0\n",
      "          Linear-384              [1, 433, 512]       1,049,088\n",
      "         Dropout-385              [1, 433, 512]               0\n",
      "             Mlp-386              [1, 433, 512]               0\n",
      "        Identity-387              [1, 433, 512]               0\n",
      "        Identity-388              [1, 433, 512]               0\n",
      "           Block-389              [1, 433, 512]               0\n",
      "       LayerNorm-390              [1, 433, 512]           1,024\n",
      "          Linear-391             [1, 433, 1536]         787,968\n",
      "        Identity-392           [1, 16, 433, 32]               0\n",
      "        Identity-393           [1, 16, 433, 32]               0\n",
      "        Identity-394              [1, 433, 512]               0\n",
      "          Linear-395              [1, 433, 512]         262,656\n",
      "         Dropout-396              [1, 433, 512]               0\n",
      "       Attention-397              [1, 433, 512]               0\n",
      "        Identity-398              [1, 433, 512]               0\n",
      "        Identity-399              [1, 433, 512]               0\n",
      "       LayerNorm-400              [1, 433, 512]           1,024\n",
      "          Linear-401             [1, 433, 2048]       1,050,624\n",
      "            GELU-402             [1, 433, 2048]               0\n",
      "         Dropout-403             [1, 433, 2048]               0\n",
      "        Identity-404             [1, 433, 2048]               0\n",
      "          Linear-405              [1, 433, 512]       1,049,088\n",
      "         Dropout-406              [1, 433, 512]               0\n",
      "             Mlp-407              [1, 433, 512]               0\n",
      "        Identity-408              [1, 433, 512]               0\n",
      "        Identity-409              [1, 433, 512]               0\n",
      "           Block-410              [1, 433, 512]               0\n",
      "       LayerNorm-411              [1, 433, 512]           1,024\n",
      "          Linear-412             [1, 433, 1536]         787,968\n",
      "        Identity-413           [1, 16, 433, 32]               0\n",
      "        Identity-414           [1, 16, 433, 32]               0\n",
      "        Identity-415              [1, 433, 512]               0\n",
      "          Linear-416              [1, 433, 512]         262,656\n",
      "         Dropout-417              [1, 433, 512]               0\n",
      "       Attention-418              [1, 433, 512]               0\n",
      "        Identity-419              [1, 433, 512]               0\n",
      "        Identity-420              [1, 433, 512]               0\n",
      "       LayerNorm-421              [1, 433, 512]           1,024\n",
      "          Linear-422             [1, 433, 2048]       1,050,624\n",
      "            GELU-423             [1, 433, 2048]               0\n",
      "         Dropout-424             [1, 433, 2048]               0\n",
      "        Identity-425             [1, 433, 2048]               0\n",
      "          Linear-426              [1, 433, 512]       1,049,088\n",
      "         Dropout-427              [1, 433, 512]               0\n",
      "             Mlp-428              [1, 433, 512]               0\n",
      "        Identity-429              [1, 433, 512]               0\n",
      "        Identity-430              [1, 433, 512]               0\n",
      "           Block-431              [1, 433, 512]               0\n",
      "       LayerNorm-432              [1, 433, 512]           1,024\n",
      "          Linear-433              [1, 144, 256]         131,328\n",
      "          Linear-434              [1, 144, 256]         131,328\n",
      "          Linear-435              [1, 144, 128]          65,664\n",
      "================================================================\n",
      "Total params: 111,491,968\n",
      "Trainable params: 111,491,968\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.35\n",
      "Forward/backward pass size (MB): 752.89\n",
      "Params size (MB): 425.31\n",
      "Estimated Total Size (MB): 1178.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = SatMAE_Pre_MS.from_pretrained('MVRL/satmae-vitbase-multispec-pretrain')\n",
    "model.in_c = 10\n",
    "summary(model = model, input_size=(10,96,96), batch_size=1, device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:29:03.679186Z",
     "start_time": "2025-09-12T21:29:02.719302Z"
    }
   },
   "id": "f21ec073c86ff96"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "features = {}\n",
    "layers = [2,6,9,12]\n",
    "for l in layers: \n",
    "    model.blocks[l - 1].register_forward_hook(lambda m, inp, out, i=l: features.setdefault(i, out))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:09.617635Z",
     "start_time": "2025-09-12T21:17:09.611291Z"
    }
   },
   "id": "730e74b6055c13f1"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "random_inp = torch.rand(1,10,96,96) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:10.998902Z",
     "start_time": "2025-09-12T21:17:10.993486Z"
    }
   },
   "id": "1736ffb1d7af056f"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "latent, mask, ids = model.forward_encoder(random_inp, mask_ratio=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:11.535844Z",
     "start_time": "2025-09-12T21:17:11.417403Z"
    }
   },
   "id": "4363f614598ee370"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 433, 768])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[12].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:11.725668Z",
     "start_time": "2025-09-12T21:17:11.722142Z"
    }
   },
   "id": "4848b6c121c66d3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c118542b5888e742"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Satmae(nn.Module): \n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            num_classes = 5,\n",
    "            TSA_scale_factor_list = [2,4,8],\n",
    "            decoder_scale_factor_list = 2,\n",
    "            decoder_in_channel_list = [384, 768, 768 * 3],\n",
    "            decoder_out_channel_list = [128, 256, 512], \n",
    "            skip_blocks = [4,7,10],\n",
    "            TSA_out_channels = [64, 128, 256], \n",
    "            TSA_in_channels = 768 * 3,\n",
    "            DoubleConv_out_channels = 32,\n",
    "            useMidChannel = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # model \n",
    "        self.pretrainedSatmae = model\n",
    "        \n",
    "        self.skip_blocks = skip_blocks\n",
    "                \n",
    "        self.decoder_scale_factor = decoder_scale_factor_list\n",
    "        \n",
    "        self.TSA_scale_factors = TSA_scale_factor_list\n",
    "        \n",
    "        self.decoder_in_c =  decoder_in_channel_list\n",
    "        \n",
    "        self.decoder_out_c = decoder_out_channel_list\n",
    "        \n",
    "        self.TSA_out_c = TSA_out_channels\n",
    "        \n",
    "        self.TSA_in_c = TSA_in_channels \n",
    "        \n",
    "        self.dc_in_c = self.TSA_out_c[-1] + self.decoder_out_c[-1]\n",
    "        \n",
    "        self.dc_out_c = DoubleConv_out_channels\n",
    "        \n",
    "        if useMidChannel:\n",
    "            self.dc_mid_channel = (self.dc_in_c + self.dc_out_c) // 2\n",
    "        \n",
    "        \n",
    "        # decoder section \n",
    "        # decoder first block \n",
    "        \n",
    "        for param in self.pretrainedSatmae.parameters(): \n",
    "            param.required_grad = False\n",
    "        \n",
    "        \n",
    "        # all the 3 decoder blocks\n",
    "        for i in range(1, 4): \n",
    "            setattr(\n",
    "                self, f\"decoder{i}\",\n",
    "                Decoder(\n",
    "                    in_channel= self.decoder_in_c[i],\n",
    "                    out_channel=self.decoder_out_c[i],\n",
    "                    scale_factor=self.decoder_scale_factor,\n",
    "                )\n",
    "            )\n",
    "            setattr(\n",
    "                self, f\"skipConnection{i}\", \n",
    "                TransformerSkipAdapter(\n",
    "                    in_channel=self.TSA_in_c,\n",
    "                    out_channel=self.TSA_out_c[i],\n",
    "                    scale_factor=self.TSA_scale_factors[i],\n",
    "                    )\n",
    "            )\n",
    "            \n",
    "        self.dc = DoubleConv(in_channel=self.dc_in_c, out_channel=self.dc_out_c, mid_channels=self.dc_mid_channel)\n",
    "        \n",
    "        self.outConv = outConv(in_channel= self.dc_out_c, out_channel=num_classes)\n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, img):\n",
    "        \n",
    "        enc_output = self.pretrainedSatmae.forward_encoder(img)\n",
    "        enc_output = self.parse_enc_output(enc_output)\n",
    "        # gives you 433 output, one is cls \n",
    "        # remove 433 to 432 \n",
    "        # 432 = 12 x 12 x 3\n",
    "\n",
    "        self.features = {}\n",
    "        for l in self.skip_blocks:\n",
    "            self.pretrainedSatmae.blocks[l - 1].register_forward_hook(\n",
    "                lambda m, inp, out, i=l: self.features.__setitem__(i, out)\n",
    "            )\n",
    "        \n",
    "        # decoder block 3 + TSA block 3 + concat \n",
    "        enc_output = self.decoder3(enc_output)\n",
    "    \n",
    "        skipFeat = features[self.skip_blocks[2]]\n",
    "        skipFeat = self.parse_enc_output(skipFeat)\n",
    "        skipFeat = self.skipConnection3(skipFeat)\n",
    "        \n",
    "        enc_output = torch.cat((enc_output, skipFeat), dim= 1)\n",
    "        \n",
    "        # decoder block 2 + TSA block 2 + concat\n",
    "        enc_output = self.decoder2(enc_output)\n",
    "        skipFeat = features[self.skip_blocks[1]]\n",
    "        skipFeat = self.parse_enc_output(skipFeat)\n",
    "        skipFeat = self.skipConnection2(skipFeat)\n",
    "        \n",
    "        enc_output = torch.cat((enc_output, skipFeat), dim= 1)\n",
    "        \n",
    "        # decoder block 1 + TSA block 1 + concat\n",
    "        enc_output = self.decoder1(enc_output)\n",
    "        skipFeat = features[skipFeat[0]]\n",
    "        skipFeat = self.parse_enc_output(skipFeat)\n",
    "        skipFeat = self.skipConnection1(skipFeat)\n",
    "        \n",
    "        enc_output = torch.cat((enc_output, skipFeat), dim= 1)\n",
    "\n",
    "        # all decoders are done \n",
    "        enc_output = self.dc(enc_output)\n",
    "        \n",
    "        # segmentation map (or output)\n",
    "        output = self.outConv(enc_output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def parse_enc_output(self, feat):\n",
    "        \n",
    "        # dropping cls tag\n",
    "        feat = feat[:, 1:, :] # (B, 432, 768)\n",
    "        \n",
    "        feat = feat.view(feat.size(0), 3, 12, 12, 768) \n",
    "        \n",
    "        feat = feat.permute(0,1,4,2,3)\n",
    "        \n",
    "        feat = feat.reshape(feat.size(0), 768 * 3, 12, 12)\n",
    "        \n",
    "        return feat\n",
    "\n",
    "\n",
    "\n",
    "    def StartFineTuning(self, blocks_to_unfreeze=11):\n",
    "        for idx in range(blocks_to_unfreeze, len(model.blocks)):\n",
    "            for p in model.blocks[-idx].parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64a2d1cfa3a0fad8"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.blocks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T01:46:22.347773Z",
     "start_time": "2025-09-14T01:46:22.338171Z"
    }
   },
   "id": "326fd25c0514feed"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'': MaskedAutoencoderGroupChannelViT(\n   (patch_embed): ModuleList(\n     (0-1): 2 x PatchEmbed(\n       (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))\n       (norm): Identity()\n     )\n     (2): PatchEmbed(\n       (proj): Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8))\n       (norm): Identity()\n     )\n   )\n   (blocks): ModuleList(\n     (0-11): 12 x Block(\n       (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n       (attn): Attention(\n         (qkv): Linear(in_features=768, out_features=2304, bias=True)\n         (q_norm): Identity()\n         (k_norm): Identity()\n         (attn_drop): Dropout(p=0.0, inplace=False)\n         (norm): Identity()\n         (proj): Linear(in_features=768, out_features=768, bias=True)\n         (proj_drop): Dropout(p=0.0, inplace=False)\n       )\n       (ls1): Identity()\n       (drop_path1): Identity()\n       (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n       (mlp): Mlp(\n         (fc1): Linear(in_features=768, out_features=3072, bias=True)\n         (act): GELU(approximate='none')\n         (drop1): Dropout(p=0.0, inplace=False)\n         (norm): Identity()\n         (fc2): Linear(in_features=3072, out_features=768, bias=True)\n         (drop2): Dropout(p=0.0, inplace=False)\n       )\n       (ls2): Identity()\n       (drop_path2): Identity()\n     )\n   )\n   (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n   (decoder_blocks): ModuleList(\n     (0-7): 8 x Block(\n       (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n       (attn): Attention(\n         (qkv): Linear(in_features=512, out_features=1536, bias=True)\n         (q_norm): Identity()\n         (k_norm): Identity()\n         (attn_drop): Dropout(p=0.0, inplace=False)\n         (norm): Identity()\n         (proj): Linear(in_features=512, out_features=512, bias=True)\n         (proj_drop): Dropout(p=0.0, inplace=False)\n       )\n       (ls1): Identity()\n       (drop_path1): Identity()\n       (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n       (mlp): Mlp(\n         (fc1): Linear(in_features=512, out_features=2048, bias=True)\n         (act): GELU(approximate='none')\n         (drop1): Dropout(p=0.0, inplace=False)\n         (norm): Identity()\n         (fc2): Linear(in_features=2048, out_features=512, bias=True)\n         (drop2): Dropout(p=0.0, inplace=False)\n       )\n       (ls2): Identity()\n       (drop_path2): Identity()\n     )\n   )\n   (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (decoder_pred): ModuleList(\n     (0-1): 2 x Linear(in_features=512, out_features=256, bias=True)\n     (2): Linear(in_features=512, out_features=128, bias=True)\n   )\n ),\n 'patch_embed': ModuleList(\n   (0-1): 2 x PatchEmbed(\n     (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))\n     (norm): Identity()\n   )\n   (2): PatchEmbed(\n     (proj): Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8))\n     (norm): Identity()\n   )\n ),\n 'patch_embed.0': PatchEmbed(\n   (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))\n   (norm): Identity()\n ),\n 'patch_embed.0.proj': Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8)),\n 'patch_embed.0.norm': Identity(),\n 'patch_embed.1': PatchEmbed(\n   (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))\n   (norm): Identity()\n ),\n 'patch_embed.1.proj': Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8)),\n 'patch_embed.1.norm': Identity(),\n 'patch_embed.2': PatchEmbed(\n   (proj): Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8))\n   (norm): Identity()\n ),\n 'patch_embed.2.proj': Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8)),\n 'patch_embed.2.norm': Identity(),\n 'blocks': ModuleList(\n   (0-11): 12 x Block(\n     (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n     (attn): Attention(\n       (qkv): Linear(in_features=768, out_features=2304, bias=True)\n       (q_norm): Identity()\n       (k_norm): Identity()\n       (attn_drop): Dropout(p=0.0, inplace=False)\n       (norm): Identity()\n       (proj): Linear(in_features=768, out_features=768, bias=True)\n       (proj_drop): Dropout(p=0.0, inplace=False)\n     )\n     (ls1): Identity()\n     (drop_path1): Identity()\n     (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n     (mlp): Mlp(\n       (fc1): Linear(in_features=768, out_features=3072, bias=True)\n       (act): GELU(approximate='none')\n       (drop1): Dropout(p=0.0, inplace=False)\n       (norm): Identity()\n       (fc2): Linear(in_features=3072, out_features=768, bias=True)\n       (drop2): Dropout(p=0.0, inplace=False)\n     )\n     (ls2): Identity()\n     (drop_path2): Identity()\n   )\n ),\n 'blocks.0': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.0.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.0.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.0.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.0.attn.q_norm': Identity(),\n 'blocks.0.attn.k_norm': Identity(),\n 'blocks.0.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.0.attn.norm': Identity(),\n 'blocks.0.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.0.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.0.ls1': Identity(),\n 'blocks.0.drop_path1': Identity(),\n 'blocks.0.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.0.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.0.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.0.mlp.act': GELU(approximate='none'),\n 'blocks.0.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.0.mlp.norm': Identity(),\n 'blocks.0.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.0.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.0.ls2': Identity(),\n 'blocks.0.drop_path2': Identity(),\n 'blocks.1': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.1.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.1.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.1.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.1.attn.q_norm': Identity(),\n 'blocks.1.attn.k_norm': Identity(),\n 'blocks.1.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.1.attn.norm': Identity(),\n 'blocks.1.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.1.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.1.ls1': Identity(),\n 'blocks.1.drop_path1': Identity(),\n 'blocks.1.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.1.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.1.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.1.mlp.act': GELU(approximate='none'),\n 'blocks.1.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.1.mlp.norm': Identity(),\n 'blocks.1.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.1.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.1.ls2': Identity(),\n 'blocks.1.drop_path2': Identity(),\n 'blocks.2': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.2.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.2.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.2.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.2.attn.q_norm': Identity(),\n 'blocks.2.attn.k_norm': Identity(),\n 'blocks.2.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.2.attn.norm': Identity(),\n 'blocks.2.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.2.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.2.ls1': Identity(),\n 'blocks.2.drop_path1': Identity(),\n 'blocks.2.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.2.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.2.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.2.mlp.act': GELU(approximate='none'),\n 'blocks.2.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.2.mlp.norm': Identity(),\n 'blocks.2.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.2.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.2.ls2': Identity(),\n 'blocks.2.drop_path2': Identity(),\n 'blocks.3': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.3.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.3.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.3.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.3.attn.q_norm': Identity(),\n 'blocks.3.attn.k_norm': Identity(),\n 'blocks.3.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.3.attn.norm': Identity(),\n 'blocks.3.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.3.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.3.ls1': Identity(),\n 'blocks.3.drop_path1': Identity(),\n 'blocks.3.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.3.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.3.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.3.mlp.act': GELU(approximate='none'),\n 'blocks.3.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.3.mlp.norm': Identity(),\n 'blocks.3.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.3.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.3.ls2': Identity(),\n 'blocks.3.drop_path2': Identity(),\n 'blocks.4': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.4.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.4.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.4.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.4.attn.q_norm': Identity(),\n 'blocks.4.attn.k_norm': Identity(),\n 'blocks.4.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.4.attn.norm': Identity(),\n 'blocks.4.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.4.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.4.ls1': Identity(),\n 'blocks.4.drop_path1': Identity(),\n 'blocks.4.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.4.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.4.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.4.mlp.act': GELU(approximate='none'),\n 'blocks.4.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.4.mlp.norm': Identity(),\n 'blocks.4.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.4.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.4.ls2': Identity(),\n 'blocks.4.drop_path2': Identity(),\n 'blocks.5': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.5.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.5.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.5.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.5.attn.q_norm': Identity(),\n 'blocks.5.attn.k_norm': Identity(),\n 'blocks.5.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.5.attn.norm': Identity(),\n 'blocks.5.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.5.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.5.ls1': Identity(),\n 'blocks.5.drop_path1': Identity(),\n 'blocks.5.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.5.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.5.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.5.mlp.act': GELU(approximate='none'),\n 'blocks.5.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.5.mlp.norm': Identity(),\n 'blocks.5.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.5.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.5.ls2': Identity(),\n 'blocks.5.drop_path2': Identity(),\n 'blocks.6': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.6.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.6.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.6.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.6.attn.q_norm': Identity(),\n 'blocks.6.attn.k_norm': Identity(),\n 'blocks.6.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.6.attn.norm': Identity(),\n 'blocks.6.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.6.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.6.ls1': Identity(),\n 'blocks.6.drop_path1': Identity(),\n 'blocks.6.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.6.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.6.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.6.mlp.act': GELU(approximate='none'),\n 'blocks.6.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.6.mlp.norm': Identity(),\n 'blocks.6.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.6.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.6.ls2': Identity(),\n 'blocks.6.drop_path2': Identity(),\n 'blocks.7': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.7.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.7.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.7.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.7.attn.q_norm': Identity(),\n 'blocks.7.attn.k_norm': Identity(),\n 'blocks.7.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.7.attn.norm': Identity(),\n 'blocks.7.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.7.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.7.ls1': Identity(),\n 'blocks.7.drop_path1': Identity(),\n 'blocks.7.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.7.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.7.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.7.mlp.act': GELU(approximate='none'),\n 'blocks.7.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.7.mlp.norm': Identity(),\n 'blocks.7.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.7.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.7.ls2': Identity(),\n 'blocks.7.drop_path2': Identity(),\n 'blocks.8': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.8.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.8.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.8.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.8.attn.q_norm': Identity(),\n 'blocks.8.attn.k_norm': Identity(),\n 'blocks.8.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.8.attn.norm': Identity(),\n 'blocks.8.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.8.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.8.ls1': Identity(),\n 'blocks.8.drop_path1': Identity(),\n 'blocks.8.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.8.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.8.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.8.mlp.act': GELU(approximate='none'),\n 'blocks.8.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.8.mlp.norm': Identity(),\n 'blocks.8.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.8.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.8.ls2': Identity(),\n 'blocks.8.drop_path2': Identity(),\n 'blocks.9': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.9.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.9.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.9.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.9.attn.q_norm': Identity(),\n 'blocks.9.attn.k_norm': Identity(),\n 'blocks.9.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.9.attn.norm': Identity(),\n 'blocks.9.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.9.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.9.ls1': Identity(),\n 'blocks.9.drop_path1': Identity(),\n 'blocks.9.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.9.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.9.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.9.mlp.act': GELU(approximate='none'),\n 'blocks.9.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.9.mlp.norm': Identity(),\n 'blocks.9.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.9.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.9.ls2': Identity(),\n 'blocks.9.drop_path2': Identity(),\n 'blocks.10': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.10.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.10.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.10.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.10.attn.q_norm': Identity(),\n 'blocks.10.attn.k_norm': Identity(),\n 'blocks.10.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.10.attn.norm': Identity(),\n 'blocks.10.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.10.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.10.ls1': Identity(),\n 'blocks.10.drop_path1': Identity(),\n 'blocks.10.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.10.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.10.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.10.mlp.act': GELU(approximate='none'),\n 'blocks.10.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.10.mlp.norm': Identity(),\n 'blocks.10.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.10.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.10.ls2': Identity(),\n 'blocks.10.drop_path2': Identity(),\n 'blocks.11': Block(\n   (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=768, out_features=2304, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=768, out_features=768, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=768, out_features=3072, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=3072, out_features=768, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'blocks.11.norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.11.attn': Attention(\n   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=768, out_features=768, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.11.attn.qkv': Linear(in_features=768, out_features=2304, bias=True),\n 'blocks.11.attn.q_norm': Identity(),\n 'blocks.11.attn.k_norm': Identity(),\n 'blocks.11.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'blocks.11.attn.norm': Identity(),\n 'blocks.11.attn.proj': Linear(in_features=768, out_features=768, bias=True),\n 'blocks.11.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'blocks.11.ls1': Identity(),\n 'blocks.11.drop_path1': Identity(),\n 'blocks.11.norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'blocks.11.mlp': Mlp(\n   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'blocks.11.mlp.fc1': Linear(in_features=768, out_features=3072, bias=True),\n 'blocks.11.mlp.act': GELU(approximate='none'),\n 'blocks.11.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'blocks.11.mlp.norm': Identity(),\n 'blocks.11.mlp.fc2': Linear(in_features=3072, out_features=768, bias=True),\n 'blocks.11.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'blocks.11.ls2': Identity(),\n 'blocks.11.drop_path2': Identity(),\n 'norm': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n 'decoder_embed': Linear(in_features=768, out_features=512, bias=True),\n 'decoder_blocks': ModuleList(\n   (0-7): 8 x Block(\n     (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n     (attn): Attention(\n       (qkv): Linear(in_features=512, out_features=1536, bias=True)\n       (q_norm): Identity()\n       (k_norm): Identity()\n       (attn_drop): Dropout(p=0.0, inplace=False)\n       (norm): Identity()\n       (proj): Linear(in_features=512, out_features=512, bias=True)\n       (proj_drop): Dropout(p=0.0, inplace=False)\n     )\n     (ls1): Identity()\n     (drop_path1): Identity()\n     (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n     (mlp): Mlp(\n       (fc1): Linear(in_features=512, out_features=2048, bias=True)\n       (act): GELU(approximate='none')\n       (drop1): Dropout(p=0.0, inplace=False)\n       (norm): Identity()\n       (fc2): Linear(in_features=2048, out_features=512, bias=True)\n       (drop2): Dropout(p=0.0, inplace=False)\n     )\n     (ls2): Identity()\n     (drop_path2): Identity()\n   )\n ),\n 'decoder_blocks.0': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.0.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.0.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.0.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.0.attn.q_norm': Identity(),\n 'decoder_blocks.0.attn.k_norm': Identity(),\n 'decoder_blocks.0.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.0.attn.norm': Identity(),\n 'decoder_blocks.0.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.0.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.0.ls1': Identity(),\n 'decoder_blocks.0.drop_path1': Identity(),\n 'decoder_blocks.0.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.0.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.0.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.0.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.0.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.0.mlp.norm': Identity(),\n 'decoder_blocks.0.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.0.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.0.ls2': Identity(),\n 'decoder_blocks.0.drop_path2': Identity(),\n 'decoder_blocks.1': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.1.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.1.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.1.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.1.attn.q_norm': Identity(),\n 'decoder_blocks.1.attn.k_norm': Identity(),\n 'decoder_blocks.1.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.1.attn.norm': Identity(),\n 'decoder_blocks.1.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.1.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.1.ls1': Identity(),\n 'decoder_blocks.1.drop_path1': Identity(),\n 'decoder_blocks.1.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.1.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.1.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.1.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.1.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.1.mlp.norm': Identity(),\n 'decoder_blocks.1.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.1.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.1.ls2': Identity(),\n 'decoder_blocks.1.drop_path2': Identity(),\n 'decoder_blocks.2': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.2.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.2.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.2.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.2.attn.q_norm': Identity(),\n 'decoder_blocks.2.attn.k_norm': Identity(),\n 'decoder_blocks.2.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.2.attn.norm': Identity(),\n 'decoder_blocks.2.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.2.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.2.ls1': Identity(),\n 'decoder_blocks.2.drop_path1': Identity(),\n 'decoder_blocks.2.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.2.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.2.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.2.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.2.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.2.mlp.norm': Identity(),\n 'decoder_blocks.2.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.2.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.2.ls2': Identity(),\n 'decoder_blocks.2.drop_path2': Identity(),\n 'decoder_blocks.3': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.3.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.3.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.3.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.3.attn.q_norm': Identity(),\n 'decoder_blocks.3.attn.k_norm': Identity(),\n 'decoder_blocks.3.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.3.attn.norm': Identity(),\n 'decoder_blocks.3.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.3.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.3.ls1': Identity(),\n 'decoder_blocks.3.drop_path1': Identity(),\n 'decoder_blocks.3.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.3.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.3.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.3.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.3.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.3.mlp.norm': Identity(),\n 'decoder_blocks.3.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.3.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.3.ls2': Identity(),\n 'decoder_blocks.3.drop_path2': Identity(),\n 'decoder_blocks.4': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.4.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.4.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.4.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.4.attn.q_norm': Identity(),\n 'decoder_blocks.4.attn.k_norm': Identity(),\n 'decoder_blocks.4.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.4.attn.norm': Identity(),\n 'decoder_blocks.4.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.4.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.4.ls1': Identity(),\n 'decoder_blocks.4.drop_path1': Identity(),\n 'decoder_blocks.4.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.4.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.4.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.4.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.4.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.4.mlp.norm': Identity(),\n 'decoder_blocks.4.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.4.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.4.ls2': Identity(),\n 'decoder_blocks.4.drop_path2': Identity(),\n 'decoder_blocks.5': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.5.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.5.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.5.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.5.attn.q_norm': Identity(),\n 'decoder_blocks.5.attn.k_norm': Identity(),\n 'decoder_blocks.5.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.5.attn.norm': Identity(),\n 'decoder_blocks.5.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.5.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.5.ls1': Identity(),\n 'decoder_blocks.5.drop_path1': Identity(),\n 'decoder_blocks.5.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.5.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.5.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.5.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.5.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.5.mlp.norm': Identity(),\n 'decoder_blocks.5.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.5.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.5.ls2': Identity(),\n 'decoder_blocks.5.drop_path2': Identity(),\n 'decoder_blocks.6': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.6.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.6.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.6.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.6.attn.q_norm': Identity(),\n 'decoder_blocks.6.attn.k_norm': Identity(),\n 'decoder_blocks.6.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.6.attn.norm': Identity(),\n 'decoder_blocks.6.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.6.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.6.ls1': Identity(),\n 'decoder_blocks.6.drop_path1': Identity(),\n 'decoder_blocks.6.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.6.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.6.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.6.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.6.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.6.mlp.norm': Identity(),\n 'decoder_blocks.6.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.6.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.6.ls2': Identity(),\n 'decoder_blocks.6.drop_path2': Identity(),\n 'decoder_blocks.7': Block(\n   (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (attn): Attention(\n     (qkv): Linear(in_features=512, out_features=1536, bias=True)\n     (q_norm): Identity()\n     (k_norm): Identity()\n     (attn_drop): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (proj): Linear(in_features=512, out_features=512, bias=True)\n     (proj_drop): Dropout(p=0.0, inplace=False)\n   )\n   (ls1): Identity()\n   (drop_path1): Identity()\n   (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n   (mlp): Mlp(\n     (fc1): Linear(in_features=512, out_features=2048, bias=True)\n     (act): GELU(approximate='none')\n     (drop1): Dropout(p=0.0, inplace=False)\n     (norm): Identity()\n     (fc2): Linear(in_features=2048, out_features=512, bias=True)\n     (drop2): Dropout(p=0.0, inplace=False)\n   )\n   (ls2): Identity()\n   (drop_path2): Identity()\n ),\n 'decoder_blocks.7.norm1': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.7.attn': Attention(\n   (qkv): Linear(in_features=512, out_features=1536, bias=True)\n   (q_norm): Identity()\n   (k_norm): Identity()\n   (attn_drop): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (proj): Linear(in_features=512, out_features=512, bias=True)\n   (proj_drop): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.7.attn.qkv': Linear(in_features=512, out_features=1536, bias=True),\n 'decoder_blocks.7.attn.q_norm': Identity(),\n 'decoder_blocks.7.attn.k_norm': Identity(),\n 'decoder_blocks.7.attn.attn_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.7.attn.norm': Identity(),\n 'decoder_blocks.7.attn.proj': Linear(in_features=512, out_features=512, bias=True),\n 'decoder_blocks.7.attn.proj_drop': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.7.ls1': Identity(),\n 'decoder_blocks.7.drop_path1': Identity(),\n 'decoder_blocks.7.norm2': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_blocks.7.mlp': Mlp(\n   (fc1): Linear(in_features=512, out_features=2048, bias=True)\n   (act): GELU(approximate='none')\n   (drop1): Dropout(p=0.0, inplace=False)\n   (norm): Identity()\n   (fc2): Linear(in_features=2048, out_features=512, bias=True)\n   (drop2): Dropout(p=0.0, inplace=False)\n ),\n 'decoder_blocks.7.mlp.fc1': Linear(in_features=512, out_features=2048, bias=True),\n 'decoder_blocks.7.mlp.act': GELU(approximate='none'),\n 'decoder_blocks.7.mlp.drop1': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.7.mlp.norm': Identity(),\n 'decoder_blocks.7.mlp.fc2': Linear(in_features=2048, out_features=512, bias=True),\n 'decoder_blocks.7.mlp.drop2': Dropout(p=0.0, inplace=False),\n 'decoder_blocks.7.ls2': Identity(),\n 'decoder_blocks.7.drop_path2': Identity(),\n 'decoder_norm': LayerNorm((512,), eps=1e-06, elementwise_affine=True),\n 'decoder_pred': ModuleList(\n   (0-1): 2 x Linear(in_features=512, out_features=256, bias=True)\n   (2): Linear(in_features=512, out_features=128, bias=True)\n ),\n 'decoder_pred.0': Linear(in_features=512, out_features=256, bias=True),\n 'decoder_pred.1': Linear(in_features=512, out_features=256, bias=True),\n 'decoder_pred.2': Linear(in_features=512, out_features=128, bias=True)}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model.named_modules())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-13T22:06:16.864454Z",
     "start_time": "2025-09-13T22:06:16.855083Z"
    }
   },
   "id": "dbbd3ca3370047c1"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-11): 12 x Block(\n    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (attn): Attention(\n      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n      (q_norm): Identity()\n      (k_norm): Identity()\n      (attn_drop): Dropout(p=0.0, inplace=False)\n      (norm): Identity()\n      (proj): Linear(in_features=768, out_features=768, bias=True)\n      (proj_drop): Dropout(p=0.0, inplace=False)\n    )\n    (ls1): Identity()\n    (drop_path1): Identity()\n    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (mlp): Mlp(\n      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n      (act): GELU(approximate='none')\n      (drop1): Dropout(p=0.0, inplace=False)\n      (norm): Identity()\n      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n      (drop2): Dropout(p=0.0, inplace=False)\n    )\n    (ls2): Identity()\n    (drop_path2): Identity()\n  )\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in range(7, len(model.blocks)):\n",
    "    for p in model.blocks[-idx].parameters(): \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-13T22:07:38.822873Z",
     "start_time": "2025-09-13T22:07:38.814117Z"
    }
   },
   "id": "96e9f918820a946"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "x = torch.randn(size=(8,256,24,24))\n",
    "y = torch.randn(size = (8,512, 24,24))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T01:38:38.110183Z",
     "start_time": "2025-09-14T01:38:38.059115Z"
    }
   },
   "id": "f6dbc4114fa41db1"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "z = torch.cat((x,y), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T01:39:09.821205Z",
     "start_time": "2025-09-14T01:39:09.811882Z"
    }
   },
   "id": "1b2c814bec34ac21"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 768, 24, 24])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T01:39:11.886973Z",
     "start_time": "2025-09-14T01:39:11.873555Z"
    }
   },
   "id": "280097b6a124d7a7"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4): \n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T01:43:34.092298Z",
     "start_time": "2025-09-14T01:43:34.070443Z"
    }
   },
   "id": "5f725234019e3869"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "319ebbdb8a28474"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "semanticsegmentation",
   "language": "python",
   "display_name": "Python (semanticsegmentation)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
