{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manedge/miniconda/envs/SemanticSegmentation/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from rshf.satmae import SatMAE_Pre_MS\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.952026Z",
     "start_time": "2025-09-12T21:28:25.818995Z"
    }
   },
   "id": "4361503907736dcb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.960512Z",
     "start_time": "2025-09-12T21:28:27.953260Z"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, mid_channels=None, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # If an intermediate channel count isn't provided, match the final width.\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channel\n",
    "\n",
    "        # Two consecutive Conv-BN-ReLU blocks with 3x3 kernels and padding=1 to keep H, W unchanged.\n",
    "        self.doubleconv = nn.Sequential(\n",
    "            # First 3x3 convolution: C_in -> mid_channels\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channel,\n",
    "                out_channels=mid_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,          # keep spatial dimensions (same-conv for 3x3)\n",
    "                bias=bias\n",
    "            ),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Second 3x3 convolution: mid_channels -> C_out\n",
    "            nn.Conv2d(\n",
    "                in_channels=mid_channels,\n",
    "                out_channels=out_channel,\n",
    "                kernel_size=3,\n",
    "                padding=1,          # keep spatial dimensions (same-conv for 3x3)\n",
    "                bias=bias\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.doubleconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TransformerSkipAdapter(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, scale_factor):\n",
    "        super().__init__()\n",
    "        # Upsample the skip feature to match decoder stage resolution\n",
    "        self.up = nn.Upsample(mode='bilinear', scale_factor=scale_factor, align_corners=True)\n",
    "        # Refine feature map after resizing\n",
    "        self.dc = DoubleConv(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, skip):\n",
    "        skip = self.up(skip)      # Resize to match decoder resolution\n",
    "        skip = self.dc(skip)      # Apply DoubleConv for refinement\n",
    "        return skip\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.960919Z",
     "start_time": "2025-09-12T21:28:27.955598Z"
    }
   },
   "id": "fd1acb1163d2412d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, scale_factor):\n",
    "        super().__init__()\n",
    "        # Reduce channels and refine features at current resolution\n",
    "        self.dc = DoubleConv(in_channel=in_channel, out_channel=out_channel)\n",
    "        # Upsample to next resolution for decoding\n",
    "        self.up = nn.Upsample(mode='bilinear', scale_factor=scale_factor, align_corners=True)\n",
    "\n",
    "    def forward(self, img_patch):\n",
    "        # Example: input = (B, 768, 14, 14)\n",
    "        img_patch = self.dc(img_patch)\n",
    "        # After DoubleConv: (B, 512, 14, 14)\n",
    "\n",
    "        img_patch = self.up(img_patch)\n",
    "        # After Upsample: (B, 512, 28, 28)\n",
    "\n",
    "        return img_patch\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.961054Z",
     "start_time": "2025-09-12T21:28:27.958719Z"
    }
   },
   "id": "a85a34c2a7ca2ea7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class outConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        # 1Ã—1 conv: channel-wise linear projection, preserves (H, W)\n",
    "        self.outconv = nn.Conv2d(in_channel, out_channel, kernel_size = 1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.outconv(image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:28:27.961993Z",
     "start_time": "2025-09-12T21:28:27.960842Z"
    }
   },
   "id": "8946dfdce4b759fb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 768, 12, 12]         197,376\n",
      "          Identity-2              [1, 144, 768]               0\n",
      "        PatchEmbed-3              [1, 144, 768]               0\n",
      "            Conv2d-4           [1, 768, 12, 12]         197,376\n",
      "          Identity-5              [1, 144, 768]               0\n",
      "        PatchEmbed-6              [1, 144, 768]               0\n",
      "            Conv2d-7           [1, 768, 12, 12]          99,072\n",
      "          Identity-8              [1, 144, 768]               0\n",
      "        PatchEmbed-9              [1, 144, 768]               0\n",
      "        LayerNorm-10              [1, 109, 768]           1,536\n",
      "           Linear-11             [1, 109, 2304]       1,771,776\n",
      "         Identity-12           [1, 16, 109, 48]               0\n",
      "         Identity-13           [1, 16, 109, 48]               0\n",
      "         Identity-14              [1, 109, 768]               0\n",
      "           Linear-15              [1, 109, 768]         590,592\n",
      "          Dropout-16              [1, 109, 768]               0\n",
      "        Attention-17              [1, 109, 768]               0\n",
      "         Identity-18              [1, 109, 768]               0\n",
      "         Identity-19              [1, 109, 768]               0\n",
      "        LayerNorm-20              [1, 109, 768]           1,536\n",
      "           Linear-21             [1, 109, 3072]       2,362,368\n",
      "             GELU-22             [1, 109, 3072]               0\n",
      "          Dropout-23             [1, 109, 3072]               0\n",
      "         Identity-24             [1, 109, 3072]               0\n",
      "           Linear-25              [1, 109, 768]       2,360,064\n",
      "          Dropout-26              [1, 109, 768]               0\n",
      "              Mlp-27              [1, 109, 768]               0\n",
      "         Identity-28              [1, 109, 768]               0\n",
      "         Identity-29              [1, 109, 768]               0\n",
      "            Block-30              [1, 109, 768]               0\n",
      "        LayerNorm-31              [1, 109, 768]           1,536\n",
      "           Linear-32             [1, 109, 2304]       1,771,776\n",
      "         Identity-33           [1, 16, 109, 48]               0\n",
      "         Identity-34           [1, 16, 109, 48]               0\n",
      "         Identity-35              [1, 109, 768]               0\n",
      "           Linear-36              [1, 109, 768]         590,592\n",
      "          Dropout-37              [1, 109, 768]               0\n",
      "        Attention-38              [1, 109, 768]               0\n",
      "         Identity-39              [1, 109, 768]               0\n",
      "         Identity-40              [1, 109, 768]               0\n",
      "        LayerNorm-41              [1, 109, 768]           1,536\n",
      "           Linear-42             [1, 109, 3072]       2,362,368\n",
      "             GELU-43             [1, 109, 3072]               0\n",
      "          Dropout-44             [1, 109, 3072]               0\n",
      "         Identity-45             [1, 109, 3072]               0\n",
      "           Linear-46              [1, 109, 768]       2,360,064\n",
      "          Dropout-47              [1, 109, 768]               0\n",
      "              Mlp-48              [1, 109, 768]               0\n",
      "         Identity-49              [1, 109, 768]               0\n",
      "         Identity-50              [1, 109, 768]               0\n",
      "            Block-51              [1, 109, 768]               0\n",
      "        LayerNorm-52              [1, 109, 768]           1,536\n",
      "           Linear-53             [1, 109, 2304]       1,771,776\n",
      "         Identity-54           [1, 16, 109, 48]               0\n",
      "         Identity-55           [1, 16, 109, 48]               0\n",
      "         Identity-56              [1, 109, 768]               0\n",
      "           Linear-57              [1, 109, 768]         590,592\n",
      "          Dropout-58              [1, 109, 768]               0\n",
      "        Attention-59              [1, 109, 768]               0\n",
      "         Identity-60              [1, 109, 768]               0\n",
      "         Identity-61              [1, 109, 768]               0\n",
      "        LayerNorm-62              [1, 109, 768]           1,536\n",
      "           Linear-63             [1, 109, 3072]       2,362,368\n",
      "             GELU-64             [1, 109, 3072]               0\n",
      "          Dropout-65             [1, 109, 3072]               0\n",
      "         Identity-66             [1, 109, 3072]               0\n",
      "           Linear-67              [1, 109, 768]       2,360,064\n",
      "          Dropout-68              [1, 109, 768]               0\n",
      "              Mlp-69              [1, 109, 768]               0\n",
      "         Identity-70              [1, 109, 768]               0\n",
      "         Identity-71              [1, 109, 768]               0\n",
      "            Block-72              [1, 109, 768]               0\n",
      "        LayerNorm-73              [1, 109, 768]           1,536\n",
      "           Linear-74             [1, 109, 2304]       1,771,776\n",
      "         Identity-75           [1, 16, 109, 48]               0\n",
      "         Identity-76           [1, 16, 109, 48]               0\n",
      "         Identity-77              [1, 109, 768]               0\n",
      "           Linear-78              [1, 109, 768]         590,592\n",
      "          Dropout-79              [1, 109, 768]               0\n",
      "        Attention-80              [1, 109, 768]               0\n",
      "         Identity-81              [1, 109, 768]               0\n",
      "         Identity-82              [1, 109, 768]               0\n",
      "        LayerNorm-83              [1, 109, 768]           1,536\n",
      "           Linear-84             [1, 109, 3072]       2,362,368\n",
      "             GELU-85             [1, 109, 3072]               0\n",
      "          Dropout-86             [1, 109, 3072]               0\n",
      "         Identity-87             [1, 109, 3072]               0\n",
      "           Linear-88              [1, 109, 768]       2,360,064\n",
      "          Dropout-89              [1, 109, 768]               0\n",
      "              Mlp-90              [1, 109, 768]               0\n",
      "         Identity-91              [1, 109, 768]               0\n",
      "         Identity-92              [1, 109, 768]               0\n",
      "            Block-93              [1, 109, 768]               0\n",
      "        LayerNorm-94              [1, 109, 768]           1,536\n",
      "           Linear-95             [1, 109, 2304]       1,771,776\n",
      "         Identity-96           [1, 16, 109, 48]               0\n",
      "         Identity-97           [1, 16, 109, 48]               0\n",
      "         Identity-98              [1, 109, 768]               0\n",
      "           Linear-99              [1, 109, 768]         590,592\n",
      "         Dropout-100              [1, 109, 768]               0\n",
      "       Attention-101              [1, 109, 768]               0\n",
      "        Identity-102              [1, 109, 768]               0\n",
      "        Identity-103              [1, 109, 768]               0\n",
      "       LayerNorm-104              [1, 109, 768]           1,536\n",
      "          Linear-105             [1, 109, 3072]       2,362,368\n",
      "            GELU-106             [1, 109, 3072]               0\n",
      "         Dropout-107             [1, 109, 3072]               0\n",
      "        Identity-108             [1, 109, 3072]               0\n",
      "          Linear-109              [1, 109, 768]       2,360,064\n",
      "         Dropout-110              [1, 109, 768]               0\n",
      "             Mlp-111              [1, 109, 768]               0\n",
      "        Identity-112              [1, 109, 768]               0\n",
      "        Identity-113              [1, 109, 768]               0\n",
      "           Block-114              [1, 109, 768]               0\n",
      "       LayerNorm-115              [1, 109, 768]           1,536\n",
      "          Linear-116             [1, 109, 2304]       1,771,776\n",
      "        Identity-117           [1, 16, 109, 48]               0\n",
      "        Identity-118           [1, 16, 109, 48]               0\n",
      "        Identity-119              [1, 109, 768]               0\n",
      "          Linear-120              [1, 109, 768]         590,592\n",
      "         Dropout-121              [1, 109, 768]               0\n",
      "       Attention-122              [1, 109, 768]               0\n",
      "        Identity-123              [1, 109, 768]               0\n",
      "        Identity-124              [1, 109, 768]               0\n",
      "       LayerNorm-125              [1, 109, 768]           1,536\n",
      "          Linear-126             [1, 109, 3072]       2,362,368\n",
      "            GELU-127             [1, 109, 3072]               0\n",
      "         Dropout-128             [1, 109, 3072]               0\n",
      "        Identity-129             [1, 109, 3072]               0\n",
      "          Linear-130              [1, 109, 768]       2,360,064\n",
      "         Dropout-131              [1, 109, 768]               0\n",
      "             Mlp-132              [1, 109, 768]               0\n",
      "        Identity-133              [1, 109, 768]               0\n",
      "        Identity-134              [1, 109, 768]               0\n",
      "           Block-135              [1, 109, 768]               0\n",
      "       LayerNorm-136              [1, 109, 768]           1,536\n",
      "          Linear-137             [1, 109, 2304]       1,771,776\n",
      "        Identity-138           [1, 16, 109, 48]               0\n",
      "        Identity-139           [1, 16, 109, 48]               0\n",
      "        Identity-140              [1, 109, 768]               0\n",
      "          Linear-141              [1, 109, 768]         590,592\n",
      "         Dropout-142              [1, 109, 768]               0\n",
      "       Attention-143              [1, 109, 768]               0\n",
      "        Identity-144              [1, 109, 768]               0\n",
      "        Identity-145              [1, 109, 768]               0\n",
      "       LayerNorm-146              [1, 109, 768]           1,536\n",
      "          Linear-147             [1, 109, 3072]       2,362,368\n",
      "            GELU-148             [1, 109, 3072]               0\n",
      "         Dropout-149             [1, 109, 3072]               0\n",
      "        Identity-150             [1, 109, 3072]               0\n",
      "          Linear-151              [1, 109, 768]       2,360,064\n",
      "         Dropout-152              [1, 109, 768]               0\n",
      "             Mlp-153              [1, 109, 768]               0\n",
      "        Identity-154              [1, 109, 768]               0\n",
      "        Identity-155              [1, 109, 768]               0\n",
      "           Block-156              [1, 109, 768]               0\n",
      "       LayerNorm-157              [1, 109, 768]           1,536\n",
      "          Linear-158             [1, 109, 2304]       1,771,776\n",
      "        Identity-159           [1, 16, 109, 48]               0\n",
      "        Identity-160           [1, 16, 109, 48]               0\n",
      "        Identity-161              [1, 109, 768]               0\n",
      "          Linear-162              [1, 109, 768]         590,592\n",
      "         Dropout-163              [1, 109, 768]               0\n",
      "       Attention-164              [1, 109, 768]               0\n",
      "        Identity-165              [1, 109, 768]               0\n",
      "        Identity-166              [1, 109, 768]               0\n",
      "       LayerNorm-167              [1, 109, 768]           1,536\n",
      "          Linear-168             [1, 109, 3072]       2,362,368\n",
      "            GELU-169             [1, 109, 3072]               0\n",
      "         Dropout-170             [1, 109, 3072]               0\n",
      "        Identity-171             [1, 109, 3072]               0\n",
      "          Linear-172              [1, 109, 768]       2,360,064\n",
      "         Dropout-173              [1, 109, 768]               0\n",
      "             Mlp-174              [1, 109, 768]               0\n",
      "        Identity-175              [1, 109, 768]               0\n",
      "        Identity-176              [1, 109, 768]               0\n",
      "           Block-177              [1, 109, 768]               0\n",
      "       LayerNorm-178              [1, 109, 768]           1,536\n",
      "          Linear-179             [1, 109, 2304]       1,771,776\n",
      "        Identity-180           [1, 16, 109, 48]               0\n",
      "        Identity-181           [1, 16, 109, 48]               0\n",
      "        Identity-182              [1, 109, 768]               0\n",
      "          Linear-183              [1, 109, 768]         590,592\n",
      "         Dropout-184              [1, 109, 768]               0\n",
      "       Attention-185              [1, 109, 768]               0\n",
      "        Identity-186              [1, 109, 768]               0\n",
      "        Identity-187              [1, 109, 768]               0\n",
      "       LayerNorm-188              [1, 109, 768]           1,536\n",
      "          Linear-189             [1, 109, 3072]       2,362,368\n",
      "            GELU-190             [1, 109, 3072]               0\n",
      "         Dropout-191             [1, 109, 3072]               0\n",
      "        Identity-192             [1, 109, 3072]               0\n",
      "          Linear-193              [1, 109, 768]       2,360,064\n",
      "         Dropout-194              [1, 109, 768]               0\n",
      "             Mlp-195              [1, 109, 768]               0\n",
      "        Identity-196              [1, 109, 768]               0\n",
      "        Identity-197              [1, 109, 768]               0\n",
      "           Block-198              [1, 109, 768]               0\n",
      "       LayerNorm-199              [1, 109, 768]           1,536\n",
      "          Linear-200             [1, 109, 2304]       1,771,776\n",
      "        Identity-201           [1, 16, 109, 48]               0\n",
      "        Identity-202           [1, 16, 109, 48]               0\n",
      "        Identity-203              [1, 109, 768]               0\n",
      "          Linear-204              [1, 109, 768]         590,592\n",
      "         Dropout-205              [1, 109, 768]               0\n",
      "       Attention-206              [1, 109, 768]               0\n",
      "        Identity-207              [1, 109, 768]               0\n",
      "        Identity-208              [1, 109, 768]               0\n",
      "       LayerNorm-209              [1, 109, 768]           1,536\n",
      "          Linear-210             [1, 109, 3072]       2,362,368\n",
      "            GELU-211             [1, 109, 3072]               0\n",
      "         Dropout-212             [1, 109, 3072]               0\n",
      "        Identity-213             [1, 109, 3072]               0\n",
      "          Linear-214              [1, 109, 768]       2,360,064\n",
      "         Dropout-215              [1, 109, 768]               0\n",
      "             Mlp-216              [1, 109, 768]               0\n",
      "        Identity-217              [1, 109, 768]               0\n",
      "        Identity-218              [1, 109, 768]               0\n",
      "           Block-219              [1, 109, 768]               0\n",
      "       LayerNorm-220              [1, 109, 768]           1,536\n",
      "          Linear-221             [1, 109, 2304]       1,771,776\n",
      "        Identity-222           [1, 16, 109, 48]               0\n",
      "        Identity-223           [1, 16, 109, 48]               0\n",
      "        Identity-224              [1, 109, 768]               0\n",
      "          Linear-225              [1, 109, 768]         590,592\n",
      "         Dropout-226              [1, 109, 768]               0\n",
      "       Attention-227              [1, 109, 768]               0\n",
      "        Identity-228              [1, 109, 768]               0\n",
      "        Identity-229              [1, 109, 768]               0\n",
      "       LayerNorm-230              [1, 109, 768]           1,536\n",
      "          Linear-231             [1, 109, 3072]       2,362,368\n",
      "            GELU-232             [1, 109, 3072]               0\n",
      "         Dropout-233             [1, 109, 3072]               0\n",
      "        Identity-234             [1, 109, 3072]               0\n",
      "          Linear-235              [1, 109, 768]       2,360,064\n",
      "         Dropout-236              [1, 109, 768]               0\n",
      "             Mlp-237              [1, 109, 768]               0\n",
      "        Identity-238              [1, 109, 768]               0\n",
      "        Identity-239              [1, 109, 768]               0\n",
      "           Block-240              [1, 109, 768]               0\n",
      "       LayerNorm-241              [1, 109, 768]           1,536\n",
      "          Linear-242             [1, 109, 2304]       1,771,776\n",
      "        Identity-243           [1, 16, 109, 48]               0\n",
      "        Identity-244           [1, 16, 109, 48]               0\n",
      "        Identity-245              [1, 109, 768]               0\n",
      "          Linear-246              [1, 109, 768]         590,592\n",
      "         Dropout-247              [1, 109, 768]               0\n",
      "       Attention-248              [1, 109, 768]               0\n",
      "        Identity-249              [1, 109, 768]               0\n",
      "        Identity-250              [1, 109, 768]               0\n",
      "       LayerNorm-251              [1, 109, 768]           1,536\n",
      "          Linear-252             [1, 109, 3072]       2,362,368\n",
      "            GELU-253             [1, 109, 3072]               0\n",
      "         Dropout-254             [1, 109, 3072]               0\n",
      "        Identity-255             [1, 109, 3072]               0\n",
      "          Linear-256              [1, 109, 768]       2,360,064\n",
      "         Dropout-257              [1, 109, 768]               0\n",
      "             Mlp-258              [1, 109, 768]               0\n",
      "        Identity-259              [1, 109, 768]               0\n",
      "        Identity-260              [1, 109, 768]               0\n",
      "           Block-261              [1, 109, 768]               0\n",
      "       LayerNorm-262              [1, 109, 768]           1,536\n",
      "          Linear-263              [1, 109, 512]         393,728\n",
      "       LayerNorm-264              [1, 433, 512]           1,024\n",
      "          Linear-265             [1, 433, 1536]         787,968\n",
      "        Identity-266           [1, 16, 433, 32]               0\n",
      "        Identity-267           [1, 16, 433, 32]               0\n",
      "        Identity-268              [1, 433, 512]               0\n",
      "          Linear-269              [1, 433, 512]         262,656\n",
      "         Dropout-270              [1, 433, 512]               0\n",
      "       Attention-271              [1, 433, 512]               0\n",
      "        Identity-272              [1, 433, 512]               0\n",
      "        Identity-273              [1, 433, 512]               0\n",
      "       LayerNorm-274              [1, 433, 512]           1,024\n",
      "          Linear-275             [1, 433, 2048]       1,050,624\n",
      "            GELU-276             [1, 433, 2048]               0\n",
      "         Dropout-277             [1, 433, 2048]               0\n",
      "        Identity-278             [1, 433, 2048]               0\n",
      "          Linear-279              [1, 433, 512]       1,049,088\n",
      "         Dropout-280              [1, 433, 512]               0\n",
      "             Mlp-281              [1, 433, 512]               0\n",
      "        Identity-282              [1, 433, 512]               0\n",
      "        Identity-283              [1, 433, 512]               0\n",
      "           Block-284              [1, 433, 512]               0\n",
      "       LayerNorm-285              [1, 433, 512]           1,024\n",
      "          Linear-286             [1, 433, 1536]         787,968\n",
      "        Identity-287           [1, 16, 433, 32]               0\n",
      "        Identity-288           [1, 16, 433, 32]               0\n",
      "        Identity-289              [1, 433, 512]               0\n",
      "          Linear-290              [1, 433, 512]         262,656\n",
      "         Dropout-291              [1, 433, 512]               0\n",
      "       Attention-292              [1, 433, 512]               0\n",
      "        Identity-293              [1, 433, 512]               0\n",
      "        Identity-294              [1, 433, 512]               0\n",
      "       LayerNorm-295              [1, 433, 512]           1,024\n",
      "          Linear-296             [1, 433, 2048]       1,050,624\n",
      "            GELU-297             [1, 433, 2048]               0\n",
      "         Dropout-298             [1, 433, 2048]               0\n",
      "        Identity-299             [1, 433, 2048]               0\n",
      "          Linear-300              [1, 433, 512]       1,049,088\n",
      "         Dropout-301              [1, 433, 512]               0\n",
      "             Mlp-302              [1, 433, 512]               0\n",
      "        Identity-303              [1, 433, 512]               0\n",
      "        Identity-304              [1, 433, 512]               0\n",
      "           Block-305              [1, 433, 512]               0\n",
      "       LayerNorm-306              [1, 433, 512]           1,024\n",
      "          Linear-307             [1, 433, 1536]         787,968\n",
      "        Identity-308           [1, 16, 433, 32]               0\n",
      "        Identity-309           [1, 16, 433, 32]               0\n",
      "        Identity-310              [1, 433, 512]               0\n",
      "          Linear-311              [1, 433, 512]         262,656\n",
      "         Dropout-312              [1, 433, 512]               0\n",
      "       Attention-313              [1, 433, 512]               0\n",
      "        Identity-314              [1, 433, 512]               0\n",
      "        Identity-315              [1, 433, 512]               0\n",
      "       LayerNorm-316              [1, 433, 512]           1,024\n",
      "          Linear-317             [1, 433, 2048]       1,050,624\n",
      "            GELU-318             [1, 433, 2048]               0\n",
      "         Dropout-319             [1, 433, 2048]               0\n",
      "        Identity-320             [1, 433, 2048]               0\n",
      "          Linear-321              [1, 433, 512]       1,049,088\n",
      "         Dropout-322              [1, 433, 512]               0\n",
      "             Mlp-323              [1, 433, 512]               0\n",
      "        Identity-324              [1, 433, 512]               0\n",
      "        Identity-325              [1, 433, 512]               0\n",
      "           Block-326              [1, 433, 512]               0\n",
      "       LayerNorm-327              [1, 433, 512]           1,024\n",
      "          Linear-328             [1, 433, 1536]         787,968\n",
      "        Identity-329           [1, 16, 433, 32]               0\n",
      "        Identity-330           [1, 16, 433, 32]               0\n",
      "        Identity-331              [1, 433, 512]               0\n",
      "          Linear-332              [1, 433, 512]         262,656\n",
      "         Dropout-333              [1, 433, 512]               0\n",
      "       Attention-334              [1, 433, 512]               0\n",
      "        Identity-335              [1, 433, 512]               0\n",
      "        Identity-336              [1, 433, 512]               0\n",
      "       LayerNorm-337              [1, 433, 512]           1,024\n",
      "          Linear-338             [1, 433, 2048]       1,050,624\n",
      "            GELU-339             [1, 433, 2048]               0\n",
      "         Dropout-340             [1, 433, 2048]               0\n",
      "        Identity-341             [1, 433, 2048]               0\n",
      "          Linear-342              [1, 433, 512]       1,049,088\n",
      "         Dropout-343              [1, 433, 512]               0\n",
      "             Mlp-344              [1, 433, 512]               0\n",
      "        Identity-345              [1, 433, 512]               0\n",
      "        Identity-346              [1, 433, 512]               0\n",
      "           Block-347              [1, 433, 512]               0\n",
      "       LayerNorm-348              [1, 433, 512]           1,024\n",
      "          Linear-349             [1, 433, 1536]         787,968\n",
      "        Identity-350           [1, 16, 433, 32]               0\n",
      "        Identity-351           [1, 16, 433, 32]               0\n",
      "        Identity-352              [1, 433, 512]               0\n",
      "          Linear-353              [1, 433, 512]         262,656\n",
      "         Dropout-354              [1, 433, 512]               0\n",
      "       Attention-355              [1, 433, 512]               0\n",
      "        Identity-356              [1, 433, 512]               0\n",
      "        Identity-357              [1, 433, 512]               0\n",
      "       LayerNorm-358              [1, 433, 512]           1,024\n",
      "          Linear-359             [1, 433, 2048]       1,050,624\n",
      "            GELU-360             [1, 433, 2048]               0\n",
      "         Dropout-361             [1, 433, 2048]               0\n",
      "        Identity-362             [1, 433, 2048]               0\n",
      "          Linear-363              [1, 433, 512]       1,049,088\n",
      "         Dropout-364              [1, 433, 512]               0\n",
      "             Mlp-365              [1, 433, 512]               0\n",
      "        Identity-366              [1, 433, 512]               0\n",
      "        Identity-367              [1, 433, 512]               0\n",
      "           Block-368              [1, 433, 512]               0\n",
      "       LayerNorm-369              [1, 433, 512]           1,024\n",
      "          Linear-370             [1, 433, 1536]         787,968\n",
      "        Identity-371           [1, 16, 433, 32]               0\n",
      "        Identity-372           [1, 16, 433, 32]               0\n",
      "        Identity-373              [1, 433, 512]               0\n",
      "          Linear-374              [1, 433, 512]         262,656\n",
      "         Dropout-375              [1, 433, 512]               0\n",
      "       Attention-376              [1, 433, 512]               0\n",
      "        Identity-377              [1, 433, 512]               0\n",
      "        Identity-378              [1, 433, 512]               0\n",
      "       LayerNorm-379              [1, 433, 512]           1,024\n",
      "          Linear-380             [1, 433, 2048]       1,050,624\n",
      "            GELU-381             [1, 433, 2048]               0\n",
      "         Dropout-382             [1, 433, 2048]               0\n",
      "        Identity-383             [1, 433, 2048]               0\n",
      "          Linear-384              [1, 433, 512]       1,049,088\n",
      "         Dropout-385              [1, 433, 512]               0\n",
      "             Mlp-386              [1, 433, 512]               0\n",
      "        Identity-387              [1, 433, 512]               0\n",
      "        Identity-388              [1, 433, 512]               0\n",
      "           Block-389              [1, 433, 512]               0\n",
      "       LayerNorm-390              [1, 433, 512]           1,024\n",
      "          Linear-391             [1, 433, 1536]         787,968\n",
      "        Identity-392           [1, 16, 433, 32]               0\n",
      "        Identity-393           [1, 16, 433, 32]               0\n",
      "        Identity-394              [1, 433, 512]               0\n",
      "          Linear-395              [1, 433, 512]         262,656\n",
      "         Dropout-396              [1, 433, 512]               0\n",
      "       Attention-397              [1, 433, 512]               0\n",
      "        Identity-398              [1, 433, 512]               0\n",
      "        Identity-399              [1, 433, 512]               0\n",
      "       LayerNorm-400              [1, 433, 512]           1,024\n",
      "          Linear-401             [1, 433, 2048]       1,050,624\n",
      "            GELU-402             [1, 433, 2048]               0\n",
      "         Dropout-403             [1, 433, 2048]               0\n",
      "        Identity-404             [1, 433, 2048]               0\n",
      "          Linear-405              [1, 433, 512]       1,049,088\n",
      "         Dropout-406              [1, 433, 512]               0\n",
      "             Mlp-407              [1, 433, 512]               0\n",
      "        Identity-408              [1, 433, 512]               0\n",
      "        Identity-409              [1, 433, 512]               0\n",
      "           Block-410              [1, 433, 512]               0\n",
      "       LayerNorm-411              [1, 433, 512]           1,024\n",
      "          Linear-412             [1, 433, 1536]         787,968\n",
      "        Identity-413           [1, 16, 433, 32]               0\n",
      "        Identity-414           [1, 16, 433, 32]               0\n",
      "        Identity-415              [1, 433, 512]               0\n",
      "          Linear-416              [1, 433, 512]         262,656\n",
      "         Dropout-417              [1, 433, 512]               0\n",
      "       Attention-418              [1, 433, 512]               0\n",
      "        Identity-419              [1, 433, 512]               0\n",
      "        Identity-420              [1, 433, 512]               0\n",
      "       LayerNorm-421              [1, 433, 512]           1,024\n",
      "          Linear-422             [1, 433, 2048]       1,050,624\n",
      "            GELU-423             [1, 433, 2048]               0\n",
      "         Dropout-424             [1, 433, 2048]               0\n",
      "        Identity-425             [1, 433, 2048]               0\n",
      "          Linear-426              [1, 433, 512]       1,049,088\n",
      "         Dropout-427              [1, 433, 512]               0\n",
      "             Mlp-428              [1, 433, 512]               0\n",
      "        Identity-429              [1, 433, 512]               0\n",
      "        Identity-430              [1, 433, 512]               0\n",
      "           Block-431              [1, 433, 512]               0\n",
      "       LayerNorm-432              [1, 433, 512]           1,024\n",
      "          Linear-433              [1, 144, 256]         131,328\n",
      "          Linear-434              [1, 144, 256]         131,328\n",
      "          Linear-435              [1, 144, 128]          65,664\n",
      "================================================================\n",
      "Total params: 111,491,968\n",
      "Trainable params: 111,491,968\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.35\n",
      "Forward/backward pass size (MB): 752.89\n",
      "Params size (MB): 425.31\n",
      "Estimated Total Size (MB): 1178.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = SatMAE_Pre_MS.from_pretrained('MVRL/satmae-vitbase-multispec-pretrain')\n",
    "model.in_c = 10\n",
    "summary(model = model, input_size=(10,96,96), batch_size=1, device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:29:03.679186Z",
     "start_time": "2025-09-12T21:29:02.719302Z"
    }
   },
   "id": "f21ec073c86ff96"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "features = {}\n",
    "layers = [2,6,9,12]\n",
    "for l in layers: \n",
    "    model.blocks[l - 1].register_forward_hook(lambda m, inp, out, i=l: features.setdefault(i, out))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:09.617635Z",
     "start_time": "2025-09-12T21:17:09.611291Z"
    }
   },
   "id": "730e74b6055c13f1"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "random_inp = torch.rand(1,10,96,96) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:10.998902Z",
     "start_time": "2025-09-12T21:17:10.993486Z"
    }
   },
   "id": "1736ffb1d7af056f"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "latent, mask, ids = model.forward_encoder(random_inp, mask_ratio=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:11.535844Z",
     "start_time": "2025-09-12T21:17:11.417403Z"
    }
   },
   "id": "4363f614598ee370"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 433, 768])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[12].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T21:17:11.725668Z",
     "start_time": "2025-09-12T21:17:11.722142Z"
    }
   },
   "id": "4848b6c121c66d3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c118542b5888e742"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class Satmae(nn.Module): \n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            num_classes = 5,\n",
    "            TSA_scale_factor_list = [8,4,2],\n",
    "            decoder_scale_factor = 2,\n",
    "            decoder_in_channel_list = [384, 768, 768 * 3],\n",
    "            decoder_out_channel_list = [128, 256, 512], \n",
    "            skip_blocks = [4,7,10],\n",
    "            TSA_out_channels = [64, 128, 256], \n",
    "            TSA_in_channels = 768 * 3,\n",
    "            DoubleConv_out_channels = 32,\n",
    "            useMidChannel = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # model \n",
    "        self.pretrainedSatmae = model\n",
    "        \n",
    "        self.skip_blocks = skip_blocks\n",
    "                \n",
    "        self.decoder_scale_factor = decoder_scale_factor\n",
    "        \n",
    "        self.TSA_scale_factors = TSA_scale_factor_list\n",
    "        \n",
    "        self.decoder_in_c =  decoder_in_channel_list\n",
    "        \n",
    "        self.decoder_out_c = decoder_out_channel_list\n",
    "        \n",
    "        self.TSA_out_c = TSA_out_channels\n",
    "        \n",
    "        self.TSA_in_c = TSA_in_channels \n",
    "        \n",
    "        self.dc_in_c = self.TSA_out_c[0] + self.decoder_out_c[0]\n",
    "        \n",
    "        self.dc_out_c = DoubleConv_out_channels\n",
    "        \n",
    "        if useMidChannel:\n",
    "            self.dc_mid_channel = (self.dc_in_c + self.dc_out_c) // 2\n",
    "        \n",
    "        \n",
    "        # decoder section \n",
    "        # decoder first block \n",
    "        \n",
    "        for param in self.pretrainedSatmae.parameters(): \n",
    "            param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        # all the 3 decoder blocks\n",
    "        for i in range(1, 4): \n",
    "            setattr(\n",
    "                self, f\"decoder{i}\",\n",
    "                Decoder(\n",
    "                    in_channel= self.decoder_in_c[i-1],\n",
    "                    out_channel=self.decoder_out_c[i-1],\n",
    "                    scale_factor=self.decoder_scale_factor,\n",
    "                )\n",
    "            )\n",
    "            setattr(\n",
    "                self, f\"skipConnection{i}\", \n",
    "                TransformerSkipAdapter(\n",
    "                    in_channel=self.TSA_in_c,\n",
    "                    out_channel=self.TSA_out_c[i-1],\n",
    "                    scale_factor=self.TSA_scale_factors[i-1],\n",
    "                    )\n",
    "            )\n",
    "            \n",
    "        self.dc = DoubleConv(in_channel=self.dc_in_c, out_channel=self.dc_out_c, mid_channels=self.dc_mid_channel)\n",
    "        \n",
    "        self.outConv = outConv(in_channel= self.dc_out_c, out_channel=num_classes)\n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, img):\n",
    "        features = {}\n",
    "        for l in self.skip_blocks:\n",
    "            self.pretrainedSatmae.blocks[l - 1].register_forward_hook(\n",
    "                lambda m, inp, out, i=l: features.setdefault(i, out)\n",
    "            )\n",
    "\n",
    "\n",
    "        enc_output = self.pretrainedSatmae.forward_encoder(img, mask_ratio=0.0)[0]\n",
    "        enc_output = self.parse_enc_output(enc_output)\n",
    "        # gives you 433 output, one is cls \n",
    "        # remove 433 to 432 \n",
    "        # 432 = 12 x 12 x 3\n",
    "\n",
    "        \n",
    "        # decoder block 3 + TSA block 3 + concat \n",
    "        enc_output = self.decoder3(enc_output)\n",
    "    \n",
    "        skipFeat = features[self.skip_blocks[2]]\n",
    "        skipFeat = self.parse_enc_output(skipFeat)\n",
    "        skipFeat = self.skipConnection3(skipFeat)\n",
    "        \n",
    "        enc_output = torch.cat((enc_output, skipFeat), dim= 1)\n",
    "        \n",
    "        # decoder block 2 + TSA block 2 + concat\n",
    "        enc_output = self.decoder2(enc_output)\n",
    "        skipFeat = features[self.skip_blocks[1]]\n",
    "        skipFeat = self.parse_enc_output(skipFeat)\n",
    "        skipFeat = self.skipConnection2(skipFeat)\n",
    "        \n",
    "        enc_output = torch.cat((enc_output, skipFeat), dim= 1)\n",
    "        \n",
    "        # decoder block 1 + TSA block 1 + concat\n",
    "        enc_output = self.decoder1(enc_output)\n",
    "        skipFeat = features[self.skip_blocks[0]]\n",
    "        skipFeat = self.parse_enc_output(skipFeat)\n",
    "        skipFeat = self.skipConnection1(skipFeat)\n",
    "        \n",
    "        enc_output = torch.cat((enc_output, skipFeat), dim= 1)\n",
    "\n",
    "        # all decoders are done \n",
    "        enc_output = self.dc(enc_output)\n",
    "        \n",
    "        # segmentation map (or output)\n",
    "        output = self.outConv(enc_output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def parse_enc_output(self, feat):\n",
    "        \n",
    "        # dropping cls tag\n",
    "        B, N, C = feat.shape\n",
    "        \n",
    "        feat = feat[:, 1:, :] # (B, 432, 768)\n",
    "        \n",
    "        streams= 3\n",
    "        \n",
    "        h = w = int(math.sqrt((N-1) // streams))\n",
    "        \n",
    "        feat = feat.view(B, streams, h, w, C) \n",
    "        \n",
    "        feat = feat.permute(0,1,4,2,3)\n",
    "        \n",
    "        feat = feat.reshape(B, C * streams, h, w)\n",
    "        \n",
    "        return feat\n",
    "\n",
    "\n",
    "\n",
    "    def StartFineTuning(self, blocks_to_unfreeze=1):\n",
    "        total = len(self.pretrainedSatmae.blocks)\n",
    "        for idx in range(total - blocks_to_unfreeze, total):\n",
    "            for p in self.pretrainedSatmae.blocks[idx].parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T02:37:08.217838Z",
     "start_time": "2025-09-14T02:37:08.215254Z"
    }
   },
   "id": "64a2d1cfa3a0fad8"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "satmae_model = Satmae(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T02:37:08.456450Z",
     "start_time": "2025-09-14T02:37:08.379768Z"
    }
   },
   "id": "319ebbdb8a28474"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "Satmae(\n  (pretrainedSatmae): MaskedAutoencoderGroupChannelViT(\n    (patch_embed): ModuleList(\n      (0-1): 2 x PatchEmbed(\n        (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))\n        (norm): Identity()\n      )\n      (2): PatchEmbed(\n        (proj): Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8))\n        (norm): Identity()\n      )\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n    )\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n    (decoder_blocks): ModuleList(\n      (0-7): 8 x Block(\n        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n          (q_norm): Identity()\n          (k_norm): Identity()\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (proj): Linear(in_features=512, out_features=512, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (ls1): Identity()\n        (drop_path1): Identity()\n        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (act): GELU(approximate='none')\n          (drop1): Dropout(p=0.0, inplace=False)\n          (norm): Identity()\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (drop2): Dropout(p=0.0, inplace=False)\n        )\n        (ls2): Identity()\n        (drop_path2): Identity()\n      )\n    )\n    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n    (decoder_pred): ModuleList(\n      (0-1): 2 x Linear(in_features=512, out_features=256, bias=True)\n      (2): Linear(in_features=512, out_features=128, bias=True)\n    )\n  )\n  (decoder1): Decoder(\n    (dc): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n  )\n  (skipConnection1): TransformerSkipAdapter(\n    (up): Upsample(scale_factor=8.0, mode='bilinear')\n    (dc): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(2304, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (decoder2): Decoder(\n    (dc): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n  )\n  (skipConnection2): TransformerSkipAdapter(\n    (up): Upsample(scale_factor=4.0, mode='bilinear')\n    (dc): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(2304, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (decoder3): Decoder(\n    (dc): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(2304, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n  )\n  (skipConnection3): TransformerSkipAdapter(\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n    (dc): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(2304, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (dc): DoubleConv(\n    (doubleconv): Sequential(\n      (0): Conv2d(192, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(112, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (outConv): outConv(\n    (outconv): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satmae_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T02:37:08.545275Z",
     "start_time": "2025-09-14T02:37:08.542733Z"
    }
   },
   "id": "9432b56c4a89c9b8"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 12, 12]         197,376\n",
      "          Identity-2             [-1, 144, 768]               0\n",
      "        PatchEmbed-3             [-1, 144, 768]               0\n",
      "            Conv2d-4          [-1, 768, 12, 12]         197,376\n",
      "          Identity-5             [-1, 144, 768]               0\n",
      "        PatchEmbed-6             [-1, 144, 768]               0\n",
      "            Conv2d-7          [-1, 768, 12, 12]          99,072\n",
      "          Identity-8             [-1, 144, 768]               0\n",
      "        PatchEmbed-9             [-1, 144, 768]               0\n",
      "        LayerNorm-10             [-1, 433, 768]           1,536\n",
      "           Linear-11            [-1, 433, 2304]       1,771,776\n",
      "         Identity-12          [-1, 16, 433, 48]               0\n",
      "         Identity-13          [-1, 16, 433, 48]               0\n",
      "         Identity-14             [-1, 433, 768]               0\n",
      "           Linear-15             [-1, 433, 768]         590,592\n",
      "          Dropout-16             [-1, 433, 768]               0\n",
      "        Attention-17             [-1, 433, 768]               0\n",
      "         Identity-18             [-1, 433, 768]               0\n",
      "         Identity-19             [-1, 433, 768]               0\n",
      "        LayerNorm-20             [-1, 433, 768]           1,536\n",
      "           Linear-21            [-1, 433, 3072]       2,362,368\n",
      "             GELU-22            [-1, 433, 3072]               0\n",
      "          Dropout-23            [-1, 433, 3072]               0\n",
      "         Identity-24            [-1, 433, 3072]               0\n",
      "           Linear-25             [-1, 433, 768]       2,360,064\n",
      "          Dropout-26             [-1, 433, 768]               0\n",
      "              Mlp-27             [-1, 433, 768]               0\n",
      "         Identity-28             [-1, 433, 768]               0\n",
      "         Identity-29             [-1, 433, 768]               0\n",
      "            Block-30             [-1, 433, 768]               0\n",
      "        LayerNorm-31             [-1, 433, 768]           1,536\n",
      "           Linear-32            [-1, 433, 2304]       1,771,776\n",
      "         Identity-33          [-1, 16, 433, 48]               0\n",
      "         Identity-34          [-1, 16, 433, 48]               0\n",
      "         Identity-35             [-1, 433, 768]               0\n",
      "           Linear-36             [-1, 433, 768]         590,592\n",
      "          Dropout-37             [-1, 433, 768]               0\n",
      "        Attention-38             [-1, 433, 768]               0\n",
      "         Identity-39             [-1, 433, 768]               0\n",
      "         Identity-40             [-1, 433, 768]               0\n",
      "        LayerNorm-41             [-1, 433, 768]           1,536\n",
      "           Linear-42            [-1, 433, 3072]       2,362,368\n",
      "             GELU-43            [-1, 433, 3072]               0\n",
      "          Dropout-44            [-1, 433, 3072]               0\n",
      "         Identity-45            [-1, 433, 3072]               0\n",
      "           Linear-46             [-1, 433, 768]       2,360,064\n",
      "          Dropout-47             [-1, 433, 768]               0\n",
      "              Mlp-48             [-1, 433, 768]               0\n",
      "         Identity-49             [-1, 433, 768]               0\n",
      "         Identity-50             [-1, 433, 768]               0\n",
      "            Block-51             [-1, 433, 768]               0\n",
      "        LayerNorm-52             [-1, 433, 768]           1,536\n",
      "           Linear-53            [-1, 433, 2304]       1,771,776\n",
      "         Identity-54          [-1, 16, 433, 48]               0\n",
      "         Identity-55          [-1, 16, 433, 48]               0\n",
      "         Identity-56             [-1, 433, 768]               0\n",
      "           Linear-57             [-1, 433, 768]         590,592\n",
      "          Dropout-58             [-1, 433, 768]               0\n",
      "        Attention-59             [-1, 433, 768]               0\n",
      "         Identity-60             [-1, 433, 768]               0\n",
      "         Identity-61             [-1, 433, 768]               0\n",
      "        LayerNorm-62             [-1, 433, 768]           1,536\n",
      "           Linear-63            [-1, 433, 3072]       2,362,368\n",
      "             GELU-64            [-1, 433, 3072]               0\n",
      "          Dropout-65            [-1, 433, 3072]               0\n",
      "         Identity-66            [-1, 433, 3072]               0\n",
      "           Linear-67             [-1, 433, 768]       2,360,064\n",
      "          Dropout-68             [-1, 433, 768]               0\n",
      "              Mlp-69             [-1, 433, 768]               0\n",
      "         Identity-70             [-1, 433, 768]               0\n",
      "         Identity-71             [-1, 433, 768]               0\n",
      "            Block-72             [-1, 433, 768]               0\n",
      "        LayerNorm-73             [-1, 433, 768]           1,536\n",
      "           Linear-74            [-1, 433, 2304]       1,771,776\n",
      "         Identity-75          [-1, 16, 433, 48]               0\n",
      "         Identity-76          [-1, 16, 433, 48]               0\n",
      "         Identity-77             [-1, 433, 768]               0\n",
      "           Linear-78             [-1, 433, 768]         590,592\n",
      "          Dropout-79             [-1, 433, 768]               0\n",
      "        Attention-80             [-1, 433, 768]               0\n",
      "         Identity-81             [-1, 433, 768]               0\n",
      "         Identity-82             [-1, 433, 768]               0\n",
      "        LayerNorm-83             [-1, 433, 768]           1,536\n",
      "           Linear-84            [-1, 433, 3072]       2,362,368\n",
      "             GELU-85            [-1, 433, 3072]               0\n",
      "          Dropout-86            [-1, 433, 3072]               0\n",
      "         Identity-87            [-1, 433, 3072]               0\n",
      "           Linear-88             [-1, 433, 768]       2,360,064\n",
      "          Dropout-89             [-1, 433, 768]               0\n",
      "              Mlp-90             [-1, 433, 768]               0\n",
      "         Identity-91             [-1, 433, 768]               0\n",
      "         Identity-92             [-1, 433, 768]               0\n",
      "            Block-93             [-1, 433, 768]               0\n",
      "        LayerNorm-94             [-1, 433, 768]           1,536\n",
      "           Linear-95            [-1, 433, 2304]       1,771,776\n",
      "         Identity-96          [-1, 16, 433, 48]               0\n",
      "         Identity-97          [-1, 16, 433, 48]               0\n",
      "         Identity-98             [-1, 433, 768]               0\n",
      "           Linear-99             [-1, 433, 768]         590,592\n",
      "         Dropout-100             [-1, 433, 768]               0\n",
      "       Attention-101             [-1, 433, 768]               0\n",
      "        Identity-102             [-1, 433, 768]               0\n",
      "        Identity-103             [-1, 433, 768]               0\n",
      "       LayerNorm-104             [-1, 433, 768]           1,536\n",
      "          Linear-105            [-1, 433, 3072]       2,362,368\n",
      "            GELU-106            [-1, 433, 3072]               0\n",
      "         Dropout-107            [-1, 433, 3072]               0\n",
      "        Identity-108            [-1, 433, 3072]               0\n",
      "          Linear-109             [-1, 433, 768]       2,360,064\n",
      "         Dropout-110             [-1, 433, 768]               0\n",
      "             Mlp-111             [-1, 433, 768]               0\n",
      "        Identity-112             [-1, 433, 768]               0\n",
      "        Identity-113             [-1, 433, 768]               0\n",
      "           Block-114             [-1, 433, 768]               0\n",
      "       LayerNorm-115             [-1, 433, 768]           1,536\n",
      "          Linear-116            [-1, 433, 2304]       1,771,776\n",
      "        Identity-117          [-1, 16, 433, 48]               0\n",
      "        Identity-118          [-1, 16, 433, 48]               0\n",
      "        Identity-119             [-1, 433, 768]               0\n",
      "          Linear-120             [-1, 433, 768]         590,592\n",
      "         Dropout-121             [-1, 433, 768]               0\n",
      "       Attention-122             [-1, 433, 768]               0\n",
      "        Identity-123             [-1, 433, 768]               0\n",
      "        Identity-124             [-1, 433, 768]               0\n",
      "       LayerNorm-125             [-1, 433, 768]           1,536\n",
      "          Linear-126            [-1, 433, 3072]       2,362,368\n",
      "            GELU-127            [-1, 433, 3072]               0\n",
      "         Dropout-128            [-1, 433, 3072]               0\n",
      "        Identity-129            [-1, 433, 3072]               0\n",
      "          Linear-130             [-1, 433, 768]       2,360,064\n",
      "         Dropout-131             [-1, 433, 768]               0\n",
      "             Mlp-132             [-1, 433, 768]               0\n",
      "        Identity-133             [-1, 433, 768]               0\n",
      "        Identity-134             [-1, 433, 768]               0\n",
      "           Block-135             [-1, 433, 768]               0\n",
      "       LayerNorm-136             [-1, 433, 768]           1,536\n",
      "          Linear-137            [-1, 433, 2304]       1,771,776\n",
      "        Identity-138          [-1, 16, 433, 48]               0\n",
      "        Identity-139          [-1, 16, 433, 48]               0\n",
      "        Identity-140             [-1, 433, 768]               0\n",
      "          Linear-141             [-1, 433, 768]         590,592\n",
      "         Dropout-142             [-1, 433, 768]               0\n",
      "       Attention-143             [-1, 433, 768]               0\n",
      "        Identity-144             [-1, 433, 768]               0\n",
      "        Identity-145             [-1, 433, 768]               0\n",
      "       LayerNorm-146             [-1, 433, 768]           1,536\n",
      "          Linear-147            [-1, 433, 3072]       2,362,368\n",
      "            GELU-148            [-1, 433, 3072]               0\n",
      "         Dropout-149            [-1, 433, 3072]               0\n",
      "        Identity-150            [-1, 433, 3072]               0\n",
      "          Linear-151             [-1, 433, 768]       2,360,064\n",
      "         Dropout-152             [-1, 433, 768]               0\n",
      "             Mlp-153             [-1, 433, 768]               0\n",
      "        Identity-154             [-1, 433, 768]               0\n",
      "        Identity-155             [-1, 433, 768]               0\n",
      "           Block-156             [-1, 433, 768]               0\n",
      "       LayerNorm-157             [-1, 433, 768]           1,536\n",
      "          Linear-158            [-1, 433, 2304]       1,771,776\n",
      "        Identity-159          [-1, 16, 433, 48]               0\n",
      "        Identity-160          [-1, 16, 433, 48]               0\n",
      "        Identity-161             [-1, 433, 768]               0\n",
      "          Linear-162             [-1, 433, 768]         590,592\n",
      "         Dropout-163             [-1, 433, 768]               0\n",
      "       Attention-164             [-1, 433, 768]               0\n",
      "        Identity-165             [-1, 433, 768]               0\n",
      "        Identity-166             [-1, 433, 768]               0\n",
      "       LayerNorm-167             [-1, 433, 768]           1,536\n",
      "          Linear-168            [-1, 433, 3072]       2,362,368\n",
      "            GELU-169            [-1, 433, 3072]               0\n",
      "         Dropout-170            [-1, 433, 3072]               0\n",
      "        Identity-171            [-1, 433, 3072]               0\n",
      "          Linear-172             [-1, 433, 768]       2,360,064\n",
      "         Dropout-173             [-1, 433, 768]               0\n",
      "             Mlp-174             [-1, 433, 768]               0\n",
      "        Identity-175             [-1, 433, 768]               0\n",
      "        Identity-176             [-1, 433, 768]               0\n",
      "           Block-177             [-1, 433, 768]               0\n",
      "       LayerNorm-178             [-1, 433, 768]           1,536\n",
      "          Linear-179            [-1, 433, 2304]       1,771,776\n",
      "        Identity-180          [-1, 16, 433, 48]               0\n",
      "        Identity-181          [-1, 16, 433, 48]               0\n",
      "        Identity-182             [-1, 433, 768]               0\n",
      "          Linear-183             [-1, 433, 768]         590,592\n",
      "         Dropout-184             [-1, 433, 768]               0\n",
      "       Attention-185             [-1, 433, 768]               0\n",
      "        Identity-186             [-1, 433, 768]               0\n",
      "        Identity-187             [-1, 433, 768]               0\n",
      "       LayerNorm-188             [-1, 433, 768]           1,536\n",
      "          Linear-189            [-1, 433, 3072]       2,362,368\n",
      "            GELU-190            [-1, 433, 3072]               0\n",
      "         Dropout-191            [-1, 433, 3072]               0\n",
      "        Identity-192            [-1, 433, 3072]               0\n",
      "          Linear-193             [-1, 433, 768]       2,360,064\n",
      "         Dropout-194             [-1, 433, 768]               0\n",
      "             Mlp-195             [-1, 433, 768]               0\n",
      "        Identity-196             [-1, 433, 768]               0\n",
      "        Identity-197             [-1, 433, 768]               0\n",
      "           Block-198             [-1, 433, 768]               0\n",
      "       LayerNorm-199             [-1, 433, 768]           1,536\n",
      "          Linear-200            [-1, 433, 2304]       1,771,776\n",
      "        Identity-201          [-1, 16, 433, 48]               0\n",
      "        Identity-202          [-1, 16, 433, 48]               0\n",
      "        Identity-203             [-1, 433, 768]               0\n",
      "          Linear-204             [-1, 433, 768]         590,592\n",
      "         Dropout-205             [-1, 433, 768]               0\n",
      "       Attention-206             [-1, 433, 768]               0\n",
      "        Identity-207             [-1, 433, 768]               0\n",
      "        Identity-208             [-1, 433, 768]               0\n",
      "       LayerNorm-209             [-1, 433, 768]           1,536\n",
      "          Linear-210            [-1, 433, 3072]       2,362,368\n",
      "            GELU-211            [-1, 433, 3072]               0\n",
      "         Dropout-212            [-1, 433, 3072]               0\n",
      "        Identity-213            [-1, 433, 3072]               0\n",
      "          Linear-214             [-1, 433, 768]       2,360,064\n",
      "         Dropout-215             [-1, 433, 768]               0\n",
      "             Mlp-216             [-1, 433, 768]               0\n",
      "        Identity-217             [-1, 433, 768]               0\n",
      "        Identity-218             [-1, 433, 768]               0\n",
      "           Block-219             [-1, 433, 768]               0\n",
      "       LayerNorm-220             [-1, 433, 768]           1,536\n",
      "          Linear-221            [-1, 433, 2304]       1,771,776\n",
      "        Identity-222          [-1, 16, 433, 48]               0\n",
      "        Identity-223          [-1, 16, 433, 48]               0\n",
      "        Identity-224             [-1, 433, 768]               0\n",
      "          Linear-225             [-1, 433, 768]         590,592\n",
      "         Dropout-226             [-1, 433, 768]               0\n",
      "       Attention-227             [-1, 433, 768]               0\n",
      "        Identity-228             [-1, 433, 768]               0\n",
      "        Identity-229             [-1, 433, 768]               0\n",
      "       LayerNorm-230             [-1, 433, 768]           1,536\n",
      "          Linear-231            [-1, 433, 3072]       2,362,368\n",
      "            GELU-232            [-1, 433, 3072]               0\n",
      "         Dropout-233            [-1, 433, 3072]               0\n",
      "        Identity-234            [-1, 433, 3072]               0\n",
      "          Linear-235             [-1, 433, 768]       2,360,064\n",
      "         Dropout-236             [-1, 433, 768]               0\n",
      "             Mlp-237             [-1, 433, 768]               0\n",
      "        Identity-238             [-1, 433, 768]               0\n",
      "        Identity-239             [-1, 433, 768]               0\n",
      "           Block-240             [-1, 433, 768]               0\n",
      "       LayerNorm-241             [-1, 433, 768]           1,536\n",
      "          Linear-242            [-1, 433, 2304]       1,771,776\n",
      "        Identity-243          [-1, 16, 433, 48]               0\n",
      "        Identity-244          [-1, 16, 433, 48]               0\n",
      "        Identity-245             [-1, 433, 768]               0\n",
      "          Linear-246             [-1, 433, 768]         590,592\n",
      "         Dropout-247             [-1, 433, 768]               0\n",
      "       Attention-248             [-1, 433, 768]               0\n",
      "        Identity-249             [-1, 433, 768]               0\n",
      "        Identity-250             [-1, 433, 768]               0\n",
      "       LayerNorm-251             [-1, 433, 768]           1,536\n",
      "          Linear-252            [-1, 433, 3072]       2,362,368\n",
      "            GELU-253            [-1, 433, 3072]               0\n",
      "         Dropout-254            [-1, 433, 3072]               0\n",
      "        Identity-255            [-1, 433, 3072]               0\n",
      "          Linear-256             [-1, 433, 768]       2,360,064\n",
      "         Dropout-257             [-1, 433, 768]               0\n",
      "             Mlp-258             [-1, 433, 768]               0\n",
      "        Identity-259             [-1, 433, 768]               0\n",
      "        Identity-260             [-1, 433, 768]               0\n",
      "           Block-261             [-1, 433, 768]               0\n",
      "       LayerNorm-262             [-1, 433, 768]           1,536\n",
      "          Conv2d-263          [-1, 512, 12, 12]      10,616,832\n",
      "     BatchNorm2d-264          [-1, 512, 12, 12]           1,024\n",
      "            ReLU-265          [-1, 512, 12, 12]               0\n",
      "          Conv2d-266          [-1, 512, 12, 12]       2,359,296\n",
      "     BatchNorm2d-267          [-1, 512, 12, 12]           1,024\n",
      "            ReLU-268          [-1, 512, 12, 12]               0\n",
      "      DoubleConv-269          [-1, 512, 12, 12]               0\n",
      "        Upsample-270          [-1, 512, 24, 24]               0\n",
      "         Decoder-271          [-1, 512, 24, 24]               0\n",
      "        Upsample-272         [-1, 2304, 24, 24]               0\n",
      "          Conv2d-273          [-1, 256, 24, 24]       5,308,416\n",
      "     BatchNorm2d-274          [-1, 256, 24, 24]             512\n",
      "            ReLU-275          [-1, 256, 24, 24]               0\n",
      "          Conv2d-276          [-1, 256, 24, 24]         589,824\n",
      "     BatchNorm2d-277          [-1, 256, 24, 24]             512\n",
      "            ReLU-278          [-1, 256, 24, 24]               0\n",
      "      DoubleConv-279          [-1, 256, 24, 24]               0\n",
      "TransformerSkipAdapter-280          [-1, 256, 24, 24]               0\n",
      "          Conv2d-281          [-1, 256, 24, 24]       1,769,472\n",
      "     BatchNorm2d-282          [-1, 256, 24, 24]             512\n",
      "            ReLU-283          [-1, 256, 24, 24]               0\n",
      "          Conv2d-284          [-1, 256, 24, 24]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 24, 24]             512\n",
      "            ReLU-286          [-1, 256, 24, 24]               0\n",
      "      DoubleConv-287          [-1, 256, 24, 24]               0\n",
      "        Upsample-288          [-1, 256, 48, 48]               0\n",
      "         Decoder-289          [-1, 256, 48, 48]               0\n",
      "        Upsample-290         [-1, 2304, 48, 48]               0\n",
      "          Conv2d-291          [-1, 128, 48, 48]       2,654,208\n",
      "     BatchNorm2d-292          [-1, 128, 48, 48]             256\n",
      "            ReLU-293          [-1, 128, 48, 48]               0\n",
      "          Conv2d-294          [-1, 128, 48, 48]         147,456\n",
      "     BatchNorm2d-295          [-1, 128, 48, 48]             256\n",
      "            ReLU-296          [-1, 128, 48, 48]               0\n",
      "      DoubleConv-297          [-1, 128, 48, 48]               0\n",
      "TransformerSkipAdapter-298          [-1, 128, 48, 48]               0\n",
      "          Conv2d-299          [-1, 128, 48, 48]         442,368\n",
      "     BatchNorm2d-300          [-1, 128, 48, 48]             256\n",
      "            ReLU-301          [-1, 128, 48, 48]               0\n",
      "          Conv2d-302          [-1, 128, 48, 48]         147,456\n",
      "     BatchNorm2d-303          [-1, 128, 48, 48]             256\n",
      "            ReLU-304          [-1, 128, 48, 48]               0\n",
      "      DoubleConv-305          [-1, 128, 48, 48]               0\n",
      "        Upsample-306          [-1, 128, 96, 96]               0\n",
      "         Decoder-307          [-1, 128, 96, 96]               0\n",
      "        Upsample-308         [-1, 2304, 96, 96]               0\n",
      "          Conv2d-309           [-1, 64, 96, 96]       1,327,104\n",
      "     BatchNorm2d-310           [-1, 64, 96, 96]             128\n",
      "            ReLU-311           [-1, 64, 96, 96]               0\n",
      "          Conv2d-312           [-1, 64, 96, 96]          36,864\n",
      "     BatchNorm2d-313           [-1, 64, 96, 96]             128\n",
      "            ReLU-314           [-1, 64, 96, 96]               0\n",
      "      DoubleConv-315           [-1, 64, 96, 96]               0\n",
      "TransformerSkipAdapter-316           [-1, 64, 96, 96]               0\n",
      "          Conv2d-317          [-1, 112, 96, 96]         193,536\n",
      "     BatchNorm2d-318          [-1, 112, 96, 96]             224\n",
      "            ReLU-319          [-1, 112, 96, 96]               0\n",
      "          Conv2d-320           [-1, 32, 96, 96]          32,256\n",
      "     BatchNorm2d-321           [-1, 32, 96, 96]              64\n",
      "            ReLU-322           [-1, 32, 96, 96]               0\n",
      "      DoubleConv-323           [-1, 32, 96, 96]               0\n",
      "          Conv2d-324            [-1, 5, 96, 96]             165\n",
      "         outConv-325            [-1, 5, 96, 96]               0\n",
      "================================================================\n",
      "Total params: 111,770,565\n",
      "Trainable params: 26,220,741\n",
      "Non-trainable params: 85,549,824\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.35\n",
      "Forward/backward pass size (MB): 1443.73\n",
      "Params size (MB): 426.37\n",
      "Estimated Total Size (MB): 1870.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(satmae_model, input_size=(10,96,96))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-14T02:37:09.538216Z",
     "start_time": "2025-09-14T02:37:08.696380Z"
    }
   },
   "id": "58dfab10d53d2a62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c058552e63eefb96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "semanticsegmentation",
   "language": "python",
   "display_name": "Python (semanticsegmentation)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
