{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:44:27.998228Z",
     "start_time": "2025-08-18T01:44:27.991807Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.functional as f\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps', index=0)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "torch.set_default_device(device=device)\n",
    "torch.get_default_device()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:25:05.299867Z",
     "start_time": "2025-08-18T03:25:05.293290Z"
    }
   },
   "id": "79e5dfdba7ff37e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss functions\n",
    "- For this code to reach optimal performance we need 2 losses that we will combine \n",
    "- One of them is Dice Loss and the other is Focal loss \n",
    "- Dice loss handles class imbalance \n",
    "- Focal loss does other stuff \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5894f7d29536267"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Focal Loss "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7f73c12e828e8b"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean', task_type='mutli-class', num_classes=5):\n",
    "        \"\"\"\n",
    "        Unified Focal Loss class for binary, multi-class, and multi-label classification tasks.\n",
    "        :param gamma: Focusing parameter, controls the strength of the modulating factor (1 - p_t)^gamma\n",
    "        :param alpha: Balancing factor, can be a scalar or a tensor for class-wise weights. If None, no class balancing is used.\n",
    "        :param reduction: Specifies the reduction method: 'none' | 'mean' | 'sum'\n",
    "        :param task_type: Specifies the type of task: 'binary', 'multi-class', or 'multi-label'\n",
    "        :param num_classes: Number of classes (only required for multi-class classification)\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.task_type = task_type\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Handle alpha for class balancing in multi-class tasks\n",
    "        if task_type == 'multi-class' and alpha is not None and isinstance(alpha, (list, torch.Tensor)):\n",
    "            assert num_classes is not None, \"num_classes must be specified for multi-class classification\"\n",
    "            if isinstance(alpha, list):\n",
    "                self.alpha = torch.Tensor(alpha)\n",
    "            else:\n",
    "                self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass to compute the Focal Loss based on the specified task type.\n",
    "        :param inputs: Predictions (logits) from the model.\n",
    "                       Shape:\n",
    "                         - binary/multi-label: (batch_size, num_classes)\n",
    "                         - multi-class: (batch_size, num_classes)\n",
    "        :param targets: Ground truth labels.\n",
    "                        Shape:\n",
    "                         - binary: (batch_size,)\n",
    "                         - multi-label: (batch_size, num_classes)\n",
    "                         - multi-class: (batch_size,)\n",
    "        \"\"\"\n",
    "        if self.task_type == 'binary':\n",
    "            return self.binary_focal_loss(inputs, targets)\n",
    "        elif self.task_type == 'multi-class':\n",
    "            return self.multi_class_focal_loss(inputs, targets)\n",
    "        elif self.task_type == 'multi-label':\n",
    "            return self.multi_label_focal_loss(inputs, targets)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported task_type '{self.task_type}'. Use 'binary', 'multi-class', or 'multi-label'.\")\n",
    "\n",
    "    def binary_focal_loss(self, inputs, targets):\n",
    "        \"\"\" Focal loss for binary classification. \"\"\"\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Compute binary cross entropy\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "\n",
    "        # Compute focal weight\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # Apply alpha if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            bce_loss = alpha_t * bce_loss\n",
    "\n",
    "        # Apply focal loss weighting\n",
    "        loss = focal_weight * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "    def multi_class_focal_loss(self, inputs, targets):\n",
    "        \"\"\" Focal loss for multi-class classification. \"\"\"\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(inputs.device)\n",
    "\n",
    "        # Convert logits to probabilities with softmax\n",
    "        probs = F.softmax(inputs, dim=1)\n",
    "\n",
    "        # One-hot encode the targets\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "\n",
    "        # Compute cross-entropy for each class\n",
    "        ce_loss = -targets_one_hot * torch.log(probs)\n",
    "\n",
    "        # Compute focal weight\n",
    "        p_t = torch.sum(probs * targets_one_hot, dim=1)  # p_t for each sample\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # Apply alpha if provided (per-class weighting)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = alpha.gather(0, targets)\n",
    "            ce_loss = alpha_t.unsqueeze(1) * ce_loss\n",
    "\n",
    "        # Apply focal loss weight\n",
    "        loss = focal_weight.unsqueeze(1) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "    def multi_label_focal_loss(self, inputs, targets):\n",
    "        \"\"\" Focal loss for multi-label classification. \"\"\"\n",
    "        probs = torch.sigmoid(inputs)\n",
    "\n",
    "        # Compute binary cross entropy\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "\n",
    "        # Compute focal weight\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # Apply alpha if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            bce_loss = alpha_t * bce_loss\n",
    "\n",
    "        # Apply focal loss weight\n",
    "        loss = focal_weight * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:27:58.068987Z",
     "start_time": "2025-08-18T03:27:58.066357Z"
    }
   },
   "id": "a573251a4d22911c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dice Loss "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41272afa9bd6e3e9"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def DiceLoss(pred, target):\n",
    "    \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "    pred: tensor with first dimension as batch\n",
    "    target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    smooth = 1.\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(tflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "    return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:28:53.695171Z",
     "start_time": "2025-08-18T03:28:53.684568Z"
    }
   },
   "id": "f7d63d25e3694f20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modules \n",
    "- We create all the necessary modules for unet to work \n",
    "- these include an Attention block (class) to be used in between the big decoder functions \n",
    "- a self attention block (class) that will be used to calculate the self attention \n",
    "- This will be used only for the final layer in the unet\n",
    "- a double convolutional block - consisting of 2 conv encoders \n",
    "- an output conv block, that takes in the n_dim, h, w image and converts it to n_class, h, w image (final conv layer) \n",
    "- a down class that dictates the flow of the downward convolution (in this case the encoder section)\n",
    "- an up class that dictates the flow of the upward convolution (or conv transpose) (in this case the decoder section)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bb717a6e73d9407"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class Double conv \n",
    "- A class that has the double convolutional section that is instrumental to the unet encoder and decoder architecture \n",
    "- we set bias to false because batchnorm has its own bias so double the bias is useless\n",
    "- `nn.Conv2d(in, out, kernel = 3, padding = 1, bias=False)` lets say the input is in x h x w \n",
    "- The output would be out x h x w (in this case)... \n",
    "- The general formula = `(h - kernel + 2p) / stride + 1 = h when stride = 1 (default)` \n",
    "- In this case the output and input shape are the same but the number of filters keeps changing. \n",
    "- Usually we want to reduce the filter count for every conv block  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d69681c7c8a4616"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module): \n",
    "    def __init__(self, in_channel, out_channel, mid_channels = None, bias = False):\n",
    "        super().__init__()\n",
    "        if not mid_channels: \n",
    "            mid_channels = out_channel\n",
    "        self.doubleconv = nn.Sequential(\n",
    "            # first convolutional layer\n",
    "            nn.Conv2d(in_channels= in_channel, out_channels= mid_channels, padding= 1, kernel_size= 3, bias= bias),\n",
    "            nn.BatchNorm2d(mid_channels), \n",
    "            nn.ReLU(inplace=True), \n",
    "            \n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(in_channels=mid_channels, out_channels=out_channel, padding=1, kernel_size=3, bias= bias),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.doubleconv(x)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:24:12.038595Z",
     "start_time": "2025-08-18T03:24:12.028379Z"
    }
   },
   "id": "709f55dcaad2825c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class Down \n",
    "- This is the class used to facilitate the encoder class\n",
    "- At the moment, after the encoder we only need to run the maxpool2d after it  \n",
    "- So we run the doubleconv layer in the down class and then run the maxpool2D after this \n",
    "- Return the maxpool output for the next down layer \n",
    "- Return the encoder output for the attention layer "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afded93dc21af9e6"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class Down(nn.Module): \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.maxpool_layer = nn.MaxPool2d(kernel_size= 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = DoubleConv(in_channel=self.in_channel, out_channel=self.out_channel)(x)\n",
    "        return self.maxpool_layer(feat) # return the maxpool output and the encoder output  \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T01:54:23.613613Z",
     "start_time": "2025-08-18T01:54:23.603154Z"
    }
   },
   "id": "fa8fb8696184f3f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Self Attention block \n",
    "- Self attention of each encoder level \n",
    "- A lot of computation required "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "211311ffafc723b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query value and key calculations \n",
    "- First to calculate attention \n",
    "- Attention = query x key \n",
    "- Our query is of the shape -> `batch_size, Channels, W, H` \n",
    "- Flatten it to -> `Batch_size, Channels, H*W` (channels is in_dim // 8)  \n",
    "- Key is also of the shape -> `batch_size, Channels, H*W` (Channels is in_dim // 8) \n",
    "- Flatten Key -> `batch_size, channels, H*W`\n",
    "- Torch.batch matrix multiplication -> `Query * key` = attention at batch level \n",
    "- To perform this batch level multiplication convert query dim to Batch_size, N, Channels \n",
    "- Attention -> `Batch_size, N, N`   \n",
    "- REMEMBER THAT ATTENTION NEEDS A SOFTMAX AT THE END\n",
    "- Now sum (attention * value) is what we need (this is essentially a V * A^T\n",
    "- Value -> `B x 64 x N` Attention permute to -> `B x N x N` (This is essentially a transpose)    \n",
    "- In matrix form that becomes `torch.bmm(value, attention.T)` (The T is only for N x N not the batch)\n",
    "- You can use `nn.permute(0,2,1)` to do that"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce05aec817ba44f"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module): \n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.channel = in_dim\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels= in_dim, out_channels= in_dim // 8, kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels= in_dim, out_channels= in_dim // 8, kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels= in_dim, out_channels= in_dim, kernel_size= 1)\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        # this is essentially query transpose without touching the batch \n",
    "        query_projection = self.query_conv(x).view(batch_size, -1, width* height).permute(0,2,1)\n",
    "        key_projection = self.key_conv(x).view(batch_size, -1, width* height)\n",
    "        \n",
    "        # energy calculation -> energy is just attention before softmax\n",
    "        energy = torch.bmm(query_projection, key_projection)\n",
    "        attention = torch.softmax(energy, dim= -1)\n",
    "        \n",
    "        # Calculate Value projection \n",
    "        value_projection = self.value_conv(x).view(batch_size, -1, width* height)\n",
    "        \n",
    "        # calculate self attention mask \n",
    "        out = torch.bmm(value_projection, attention.permute(0,2,1))\n",
    "        \n",
    "        # reshape mask to fit width, height \n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        \n",
    "        # gamma stuff \n",
    "        out = self.gamma * out + x\n",
    "        \n",
    "        return out, attention\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:14:55.997496Z",
     "start_time": "2025-08-18T02:14:55.989424Z"
    }
   },
   "id": "4a16fe839a566f86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Block \n",
    "- gate (g) = decoder feature - f_g channel count of decoder  \n",
    "- x (skip) = encoder level - f_x channel count of skip level encoder \n",
    "- F_int = reduced channel count which is f_x // 2\n",
    "- weight of g => a conv block f_g to f_int with kernel, stride = 1, padding =0, bias = False (because batchnorm is true)\n",
    "- Same for x\n",
    "- Add the conv x and conv g with a relu  \n",
    "- a conv layer f_int to 1 \n",
    "- batch norm then sigmoid "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f2f6bb02c3038cc"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class Attention_block(nn.Module): \n",
    "    def __init__(self, encoder_channels, decoder_channels, intermediate_channels = None):\n",
    "        super().__init__()\n",
    "        if not intermediate_channels: \n",
    "            intermediate_channels = decoder_channels // 2\n",
    "        \n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= decoder_channels, out_channels= intermediate_channels, kernel_size= 1, stride=1, padding= 0),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "        )\n",
    "        \n",
    "        self.W_encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= encoder_channels, out_channels= intermediate_channels, kernel_size= 1, stride= 1, padding= 0),\n",
    "            nn.BatchNorm2d(intermediate_channels),\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= intermediate_channels, out_channels= 1, kernel_size= 1, stride = 1, padding= 0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, enc, dec):\n",
    "        dec_conv = self.W_gate(dec)\n",
    "        enc_conv = self.W_encoder(enc)\n",
    "        \n",
    "        psi = self.relu(dec_conv + enc_conv)\n",
    "        psi = self.psi(psi)\n",
    "        \n",
    "        return enc * psi\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:14:56.345907Z",
     "start_time": "2025-08-18T02:14:56.343977Z"
    }
   },
   "id": "565777b24e30f6cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Up function for self attention \n",
    "- This is the class used to facilitate the decoder class \n",
    "- The decoder class uses `nn.Convtranspose2D(inchannel, outchannel, kernel_size = 2, stride = 2)` which ensures that it goes from in x h x w -> out x 128 x 128 \n",
    "- There are 2 versions, one with x1 and x2. We experiment \n",
    "- Then we need to concat the encoder and the decoder output \n",
    "- Uses SELF attention modules \n",
    "- Can be used for ablation studies\n",
    "- Cannot be used for the hybrid model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bd66236c98f570e"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class self_attention_Up(nn.Module): \n",
    "    def __init__(self, in_channel, out_channel, bilinear= False):\n",
    "        super().__init__()\n",
    "        if bilinear: \n",
    "            self.up = nn.Upsample(scale_factor= 2, mode= 'bilinear', align_corners= True)\n",
    "            self.conv = DoubleConv(in_channel= in_channel, out_channel= out_channel, mid_channels=in_channel // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels= in_channel, out_channels= in_channel // 2, kernel_size= 2, \n",
    "               stride= 2)\n",
    "            self.conv = DoubleConv(in_channel= in_channel, out_channel= out_channel) # NOTE here you pass the in channel as the original in channel which might work if decoder has like 64 channel and encoder has 32 channel \n",
    "        \n",
    "    def forward(self, decoder, encoder):\n",
    "        # here encoder is the output of the encoder at that level of the unet \n",
    "        # decoder is the output of that we are supposed to feed into the decoder,\n",
    "        # usually from the previous decoder segment or the code layer\n",
    "        decoder = self.up(decoder)\n",
    "        decoder = torch.cat([decoder, encoder], dim=1)\n",
    "        \n",
    "        return self.conv(decoder) \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:14:56.662594Z",
     "start_time": "2025-08-18T02:14:56.660508Z"
    }
   },
   "id": "2eefbd3ddeb0a363"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Up function for attenion block unet \n",
    "- This is the up function for the attention block unet \n",
    "- The reason for writing this seperately is because the previous code concated the attention masked decoder \n",
    "- Directly Considering bilinear = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859ba2c92741339"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class Attention_block_up_class(nn.Module): \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode= 'bilinear', align_corners= True),\n",
    "            DoubleConv(in_channel=in_channel, out_channel=out_channel),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:22:44.277460Z",
     "start_time": "2025-08-18T03:22:44.273517Z"
    }
   },
   "id": "e31fc4dc8440a801"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Out convolutional Layer \n",
    "- THe final convolutional layer "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a75e53cdb04148"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class outConv(nn.Module): \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels= in_channel, out_channels= out_channel, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:14:56.955247Z",
     "start_time": "2025-08-18T02:14:56.952174Z"
    }
   },
   "id": "5a2af5e80dec818b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UNET class \n",
    "- Use the above functions to code a unet with attention \n",
    "- Down class for the encoder\n",
    "- Up class for the decoder\n",
    "- attention block class to calculate attention \n",
    "- worry about self attention later  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d40ce40876ca6c98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder Class\n",
    "- Unet has 4 encoder channels, 4 double conv blocks that is (in our code the class called down)\n",
    "- first block = 3 -> 64 (3 channels to 64 channels {64 filters})\n",
    "- Second block = 64 -> 128\n",
    "- Third block = 128 -> 256 \n",
    "- Fourth block = 256 -> 512\n",
    "- Fifth block = 512 -> 1024 `CODE BLOCK`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1571b623b780ed74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Self Attention block - NOT IMPLEMENTED YET \n",
    "- Here we use the self attention block to calculate self attention at the code layer  \n",
    "- We can technically expand self attention to all the layers but it is extremely memory intensive and only needed for like really clean datasets "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bab63d6811d734b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder Layer \n",
    "- This layer is also known as the class up in our code. \n",
    "- Takes the encoded image and decodes. \n",
    "- While it decodes we will feed it attention masked skip encoder values \n",
    "- This is done using the attention block function, returns the mask \n",
    "- Each block reduces the filter count \n",
    "- upconv1 => 512 in channels -> 256 out channels \n",
    "- upconv2 => 256 in channels -> 128 out channels\n",
    "- upconv3 => 128 in channels -> 64 out channels \n",
    "- upconv4 => 64 out channels -> `n_classes` out channels \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce16d9b87a90b818"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### NOTE \n",
    "taking an argument for the mid class layers might make this code more changeable "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c215a9f81d9635aa"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Self_Attention_Unet(nn.Module): \n",
    "    def __init__(\n",
    "            self,\n",
    "            image_channels,\n",
    "            n_classes,\n",
    "            mid_layers = [64,128, 256, 512, 1024], \n",
    "            bilinear= True,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Encoder section of the network\n",
    "        factor = 2 if bilinear else 1\n",
    "        \n",
    "        self.down1 = Down(image_channels, mid_layers[0]) # first layer -> input channel = 3, output channel = 64\n",
    "        self.down2 = Down(mid_layers[0], mid_layers[1]) # second layer -> input channel = 64, output channel = 128\n",
    "        self.down3 = Down(mid_layers[1], mid_layers[2])\n",
    "        self.down4 = Down(mid_layers[2], mid_layers[3])\n",
    "        \n",
    "        # calculate all the self attention values \n",
    "        self.attention1 = SelfAttention(mid_layers[0])\n",
    "        self.attention2 = SelfAttention(mid_layers[1])\n",
    "        self.attention3 = SelfAttention(mid_layers[2])\n",
    "        self.attention4 = SelfAttention(mid_layers[3])\n",
    "        \n",
    "        \n",
    "        # code block \n",
    "        self.down_code = Down(mid_layers[3], mid_layers[4] // factor)\n",
    "        \n",
    "        # self attention \n",
    "        # ADD SELF ATTENTION for code layer \n",
    "        \n",
    "        \n",
    "        \n",
    "        # decoder block\n",
    "        # if you use bilinear section, the conv2DTranspose is not invoked in favor of upsampling \n",
    "        # this means that you need the conv block to reduce the number of filters \n",
    "        # here is where we introduce a term called factor which is equal to the scaling factor \n",
    "        # in this case it is 2 \n",
    "        # that is the factor variable defined in the beginning\n",
    "        \n",
    "        self.up1 = self_attention_Up(mid_layers[4], mid_layers[3] // factor, bilinear)\n",
    "        self.up2 = self_attention_Up(mid_layers[3], mid_layers[2] // factor, bilinear)\n",
    "        self.up3 = self_attention_Up(mid_layers[2], mid_layers[1] // factor, bilinear)\n",
    "        self.up4 = self_attention_Up(mid_layers[1], mid_layers[0] // factor, bilinear)\n",
    "        \n",
    "        # outputlayers\n",
    "        self.outc = outConv(mid_layers[0] // factor, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # you cant create a seperate encoder, decoder function because you need the x1, x2 and so on values for attention calculation   \n",
    "        # encoder calculations \n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down_code(x4)\n",
    "        \n",
    "        # x5 is the code layer\n",
    "        \n",
    "        # calculate attention\n",
    "        \n",
    "        v1, _ = self.attention1(x1) \n",
    "        v2, _ = self.attention2(x2)\n",
    "        v3, _ = self.attention3(x3)\n",
    "        v4, _ = self.attention4(x4)\n",
    "        \n",
    "    \n",
    "        # Decoding \n",
    "        x = self.up1(x5, v4) # code layer and the final layer of attention \n",
    "        x = self.up2(x, v3) # decoded layer 1 (which is x) and 2nd to final layer of attention\n",
    "        x = self.up3(x, v2)\n",
    "        x = self.up4(x, v1)\n",
    "        \n",
    "        # calculate output\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:20:33.058871Z",
     "start_time": "2025-08-18T02:20:33.052682Z"
    }
   },
   "id": "5264d1a0699ef7c6"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "model = Self_Attention_Unet(3,5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:22:27.479856Z",
     "start_time": "2025-08-18T02:22:26.897317Z"
    }
   },
   "id": "6bbf1ff2f493f427"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "Self_Attention_Unet(\n  (down1): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (down2): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (down3): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (down4): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (attention1): SelfAttention(\n    (query_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n    (key_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n    (value_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (attention2): SelfAttention(\n    (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n    (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n    (value_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (attention3): SelfAttention(\n    (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n    (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n    (value_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (attention4): SelfAttention(\n    (query_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n    (key_conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n    (value_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (down_code): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (up1): self_attention_Up(\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n    (conv): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2): self_attention_Up(\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n    (conv): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3): self_attention_Up(\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n    (conv): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4): self_attention_Up(\n    (up): Upsample(scale_factor=2.0, mode='bilinear')\n    (conv): DoubleConv(\n      (doubleconv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc): outConv(\n    (conv): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T02:20:40.727047Z",
     "start_time": "2025-08-18T02:20:40.720400Z"
    }
   },
   "id": "53c8bfbebc07189a"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class Unet_with_attention_block(nn.Module): \n",
    "    def __init__(\n",
    "            self, \n",
    "            image_channels, \n",
    "            n_classes, \n",
    "            mid_layers = [64, 128, 256, 512, 1024],\n",
    "            bilinear = True      \n",
    "            ):\n",
    "        super().__init__()\n",
    "        # we dont need factor here because of how the attention layer is coded \n",
    "    \n",
    "        # Encoder \n",
    "        self.Conv1 = Down(in_channel= image_channels, out_channel= mid_layers[0])\n",
    "        self.Conv2 = Down(in_channel= mid_layers[0], out_channel= mid_layers[1])\n",
    "        self.Conv3 = Down(in_channel= mid_layers[1], out_channel= mid_layers[2])\n",
    "        self.Conv4 = Down(in_channel= mid_layers[2], out_channel= mid_layers[3])\n",
    "        self.Conv_code = Down(in_channel= mid_layers[3], out_channel= mid_layers[4])\n",
    "        \n",
    "        # decoder \n",
    "        # Upsampling including attention, then convolutional layer to reduce the size \n",
    "        # this is one block \n",
    "        self.up5 = Attention_block_up_class(in_channel= mid_layers[4], out_channel= mid_layers[3])\n",
    "        self.att5 = Attention_block(encoder_channels= mid_layers[3], decoder_channels= mid_layers[3])\n",
    "        self.up_conv5 = DoubleConv(in_channel= mid_layers[4], out_channel= mid_layers[3])\n",
    "        \n",
    "        # second upsampling \n",
    "        self.up4 = Attention_block_up_class(in_channel= mid_layers[3], out_channel= mid_layers[2])\n",
    "        self.att4 = Attention_block(encoder_channels= mid_layers[2], decoder_channels=mid_layers[2])\n",
    "        self.up_conv4 = DoubleConv(in_channel= mid_layers[3], out_channel=mid_layers[2])\n",
    "         \n",
    "        # Third upsampling \n",
    "        self.up3 = Attention_block_up_class(in_channel= mid_layers[2], out_channel= mid_layers[1])\n",
    "        self.att3 = Attention_block(encoder_channels= mid_layers[1], decoder_channels=mid_layers[1])\n",
    "        self.up_conv3 = DoubleConv(in_channel= mid_layers[2], out_channel= mid_layers[1])\n",
    "        \n",
    "        # Fourth upsampling \n",
    "        self.up2 = Attention_block_up_class(in_channel= mid_layers[1], out_channel= mid_layers[0])\n",
    "        self.att2 = Attention_block(encoder_channels= mid_layers[0], decoder_channels= mid_layers[0])\n",
    "        self.up_conv2 = DoubleConv(in_channel= mid_layers[1], out_channel= mid_layers[0])\n",
    "        \n",
    "        # final output convolutional block \n",
    "        self.outc= outConv(mid_layers[0], n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder traversal\n",
    "        x1 = self.Conv1(x)\n",
    "        x2 = self.Conv2(x1)\n",
    "        x3 = self.Conv3(x2)\n",
    "        x4 = self.Conv4(x3)\n",
    "        x5 = self.Conv_code(x4)\n",
    "        \n",
    "        # Decoding and attention concat\n",
    "        # Upsample x5, attention w respect to x4 and d5 \n",
    "        # concat \n",
    "        \n",
    "        # first block\n",
    "        d5 = self.up5(x5)\n",
    "        x4 = self.att5(d5, x4) # calculate attention block \n",
    "        d5 = torch.cat((d5, x4), dim=1)\n",
    "        d5 = self.up_conv5(d5)\n",
    "        \n",
    "        # second block \n",
    "        d4 = self.up4(d5)\n",
    "        x3 = self.att4(d4, x3) # calculate attention block \n",
    "        d4 = torch.cat((d4, x3), dim=1)\n",
    "        d4 = self.up_conv4(d4)\n",
    "\n",
    "        # third block\n",
    "        d3 = self.up3(d4)\n",
    "        x2 = self.att3(d3, x2) # calculate attention block \n",
    "        d3 = torch.cat((d3, x2), dim=1)\n",
    "        d3 = self.up_conv3(d3)\n",
    "        \n",
    "        # second block \n",
    "        d2 = self.up2(d3)\n",
    "        x1 = self.att2(d2, x1) # calculate attention block \n",
    "        d2 = torch.cat((d2, x1), dim=1)\n",
    "        d2 = self.up_conv2(d2)\n",
    "\n",
    "        logits = self.outc(d2)\n",
    "        \n",
    "        return logits \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:24:14.567950Z",
     "start_time": "2025-08-18T03:24:14.564304Z"
    }
   },
   "id": "68ec18db94f5eae"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "Unet_with_attention_block(\n  (Conv1): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (Conv2): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (Conv3): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (Conv4): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (Conv_code): Down(\n    (maxpool_layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (up5): Attention_block_up_class(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='bilinear')\n      (1): DoubleConv(\n        (doubleconv): Sequential(\n          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (att5): Attention_block(\n    (W_gate): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_encoder): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (up_conv5): DoubleConv(\n    (doubleconv): Sequential(\n      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (up4): Attention_block_up_class(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='bilinear')\n      (1): DoubleConv(\n        (doubleconv): Sequential(\n          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (att4): Attention_block(\n    (W_gate): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_encoder): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (up_conv4): DoubleConv(\n    (doubleconv): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (up3): Attention_block_up_class(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='bilinear')\n      (1): DoubleConv(\n        (doubleconv): Sequential(\n          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (att3): Attention_block(\n    (W_gate): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_encoder): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (up_conv3): DoubleConv(\n    (doubleconv): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (up2): Attention_block_up_class(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='bilinear')\n      (1): DoubleConv(\n        (doubleconv): Sequential(\n          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (att2): Attention_block(\n    (W_gate): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_encoder): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n      (1): Sigmoid()\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (up_conv2): DoubleConv(\n    (doubleconv): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (outc): outConv(\n    (conv): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Unet_with_attention_block(image_channels=6, n_classes=5)\n",
    "model "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:24:14.809048Z",
     "start_time": "2025-08-18T03:24:14.741877Z"
    }
   },
   "id": "3048a1388b1b9b1d"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         MaxPool2d-1         [-1, 64, 128, 128]               0\n",
      "              Down-2         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-3          [-1, 128, 64, 64]               0\n",
      "              Down-4          [-1, 128, 64, 64]               0\n",
      "         MaxPool2d-5          [-1, 256, 32, 32]               0\n",
      "              Down-6          [-1, 256, 32, 32]               0\n",
      "         MaxPool2d-7          [-1, 512, 16, 16]               0\n",
      "              Down-8          [-1, 512, 16, 16]               0\n",
      "         MaxPool2d-9           [-1, 1024, 8, 8]               0\n",
      "             Down-10           [-1, 1024, 8, 8]               0\n",
      "         Upsample-11         [-1, 1024, 16, 16]               0\n",
      "           Conv2d-12          [-1, 512, 16, 16]       4,718,592\n",
      "      BatchNorm2d-13          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-14          [-1, 512, 16, 16]               0\n",
      "           Conv2d-15          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-16          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-17          [-1, 512, 16, 16]               0\n",
      "       DoubleConv-18          [-1, 512, 16, 16]               0\n",
      "Attention_block_up_class-19          [-1, 512, 16, 16]               0\n",
      "           Conv2d-20          [-1, 256, 16, 16]         131,328\n",
      "      BatchNorm2d-21          [-1, 256, 16, 16]             512\n",
      "           Conv2d-22          [-1, 256, 16, 16]         131,328\n",
      "      BatchNorm2d-23          [-1, 256, 16, 16]             512\n",
      "             ReLU-24          [-1, 256, 16, 16]               0\n",
      "           Conv2d-25            [-1, 1, 16, 16]             257\n",
      "          Sigmoid-26            [-1, 1, 16, 16]               0\n",
      "  Attention_block-27          [-1, 512, 16, 16]               0\n",
      "           Conv2d-28          [-1, 512, 16, 16]       4,718,592\n",
      "      BatchNorm2d-29          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-30          [-1, 512, 16, 16]               0\n",
      "           Conv2d-31          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-32          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-33          [-1, 512, 16, 16]               0\n",
      "       DoubleConv-34          [-1, 512, 16, 16]               0\n",
      "         Upsample-35          [-1, 512, 32, 32]               0\n",
      "           Conv2d-36          [-1, 256, 32, 32]       1,179,648\n",
      "      BatchNorm2d-37          [-1, 256, 32, 32]             512\n",
      "             ReLU-38          [-1, 256, 32, 32]               0\n",
      "           Conv2d-39          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 32, 32]             512\n",
      "             ReLU-41          [-1, 256, 32, 32]               0\n",
      "       DoubleConv-42          [-1, 256, 32, 32]               0\n",
      "Attention_block_up_class-43          [-1, 256, 32, 32]               0\n",
      "           Conv2d-44          [-1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-45          [-1, 128, 32, 32]             256\n",
      "           Conv2d-46          [-1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-47          [-1, 128, 32, 32]             256\n",
      "             ReLU-48          [-1, 128, 32, 32]               0\n",
      "           Conv2d-49            [-1, 1, 32, 32]             129\n",
      "          Sigmoid-50            [-1, 1, 32, 32]               0\n",
      "  Attention_block-51          [-1, 256, 32, 32]               0\n",
      "           Conv2d-52          [-1, 256, 32, 32]       1,179,648\n",
      "      BatchNorm2d-53          [-1, 256, 32, 32]             512\n",
      "             ReLU-54          [-1, 256, 32, 32]               0\n",
      "           Conv2d-55          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-56          [-1, 256, 32, 32]             512\n",
      "             ReLU-57          [-1, 256, 32, 32]               0\n",
      "       DoubleConv-58          [-1, 256, 32, 32]               0\n",
      "         Upsample-59          [-1, 256, 64, 64]               0\n",
      "           Conv2d-60          [-1, 128, 64, 64]         294,912\n",
      "      BatchNorm2d-61          [-1, 128, 64, 64]             256\n",
      "             ReLU-62          [-1, 128, 64, 64]               0\n",
      "           Conv2d-63          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 64, 64]             256\n",
      "             ReLU-65          [-1, 128, 64, 64]               0\n",
      "       DoubleConv-66          [-1, 128, 64, 64]               0\n",
      "Attention_block_up_class-67          [-1, 128, 64, 64]               0\n",
      "           Conv2d-68           [-1, 64, 64, 64]           8,256\n",
      "      BatchNorm2d-69           [-1, 64, 64, 64]             128\n",
      "           Conv2d-70           [-1, 64, 64, 64]           8,256\n",
      "      BatchNorm2d-71           [-1, 64, 64, 64]             128\n",
      "             ReLU-72           [-1, 64, 64, 64]               0\n",
      "           Conv2d-73            [-1, 1, 64, 64]              65\n",
      "          Sigmoid-74            [-1, 1, 64, 64]               0\n",
      "  Attention_block-75          [-1, 128, 64, 64]               0\n",
      "           Conv2d-76          [-1, 128, 64, 64]         294,912\n",
      "      BatchNorm2d-77          [-1, 128, 64, 64]             256\n",
      "             ReLU-78          [-1, 128, 64, 64]               0\n",
      "           Conv2d-79          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-80          [-1, 128, 64, 64]             256\n",
      "             ReLU-81          [-1, 128, 64, 64]               0\n",
      "       DoubleConv-82          [-1, 128, 64, 64]               0\n",
      "         Upsample-83        [-1, 128, 128, 128]               0\n",
      "           Conv2d-84         [-1, 64, 128, 128]          73,728\n",
      "      BatchNorm2d-85         [-1, 64, 128, 128]             128\n",
      "             ReLU-86         [-1, 64, 128, 128]               0\n",
      "           Conv2d-87         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-88         [-1, 64, 128, 128]             128\n",
      "             ReLU-89         [-1, 64, 128, 128]               0\n",
      "       DoubleConv-90         [-1, 64, 128, 128]               0\n",
      "Attention_block_up_class-91         [-1, 64, 128, 128]               0\n",
      "           Conv2d-92         [-1, 32, 128, 128]           2,080\n",
      "      BatchNorm2d-93         [-1, 32, 128, 128]              64\n",
      "           Conv2d-94         [-1, 32, 128, 128]           2,080\n",
      "      BatchNorm2d-95         [-1, 32, 128, 128]              64\n",
      "             ReLU-96         [-1, 32, 128, 128]               0\n",
      "           Conv2d-97          [-1, 1, 128, 128]              33\n",
      "          Sigmoid-98          [-1, 1, 128, 128]               0\n",
      "  Attention_block-99         [-1, 64, 128, 128]               0\n",
      "          Conv2d-100         [-1, 64, 128, 128]          73,728\n",
      "     BatchNorm2d-101         [-1, 64, 128, 128]             128\n",
      "            ReLU-102         [-1, 64, 128, 128]               0\n",
      "          Conv2d-103         [-1, 64, 128, 128]          36,864\n",
      "     BatchNorm2d-104         [-1, 64, 128, 128]             128\n",
      "            ReLU-105         [-1, 64, 128, 128]               0\n",
      "      DoubleConv-106         [-1, 64, 128, 128]               0\n",
      "          Conv2d-107          [-1, 5, 128, 128]             325\n",
      "         outConv-108          [-1, 5, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 19,160,169\n",
      "Trainable params: 19,160,169\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.50\n",
      "Forward/backward pass size (MB): 340.08\n",
      "Params size (MB): 73.09\n",
      "Estimated Total Size (MB): 414.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(6, 256,256))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T03:24:15.946140Z",
     "start_time": "2025-08-18T03:24:15.511042Z"
    }
   },
   "id": "6fa5a858b3d2c1ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "25ed2dc8dbbd902d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "semanticsegmentation",
   "language": "python",
   "display_name": "Python (semanticsegmentation)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
